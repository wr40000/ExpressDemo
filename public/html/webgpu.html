<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="http://139.159.253.241:3000/stylesheets/github.css">
    <title>Document</title>
</head>
<body>
    <h1>orillision</h1>
<p><img src="webgpu.assets/image-20230514161145048.png" alt="image-20230514161145048"></p>
<p><img src="webgpu.assets/image-20230514161312248.png" alt="image-20230514161312248"></p>
<p><img src="webgpu.assets/image-20230514170346750.png" alt="image-20230514170346750"></p>
<p><img src="webgpu.assets/image-20230608162140442.png" alt="image-20230608162140442"></p>
<p><img src="webgpu.assets/image-20230608162210328.png" alt="image-20230608162210328"></p>
<h2>顶点插槽图解</h2>
<p><img src="webgpu.assets/image-20230817202657670.png" alt="image-20230817202657670"></p>
<p><img src="webgpu.assets/image-20230609143817409.png" alt="image-20230609143817409"></p>
<p><img src="webgpu.assets/image-20230609145352531.png" alt="image-20230609145352531"></p>
<p>GPURenderPipelineDescriptor是WebGPU API中的一个对象，用于描述渲染管线的配置。</p>
<p>在WebGPU中，渲染管线是一系列操作的集合，用于将顶点和纹理等图形数据转换成最终的像素颜色。渲染管线包括多个阶段，包括顶点输入、顶点着色器、光栅化、像素着色器和输出合成等。</p>
<p>GPURenderPipelineDescriptor对象包含了渲染管线的各个阶段的配置信息，包括顶点输入、顶点着色器、像素着色器、深度测试、模板测试等等。通过配置GPURenderPipelineDescriptor对象，可以创建一个渲染管线对象，用于进行图形渲染操作。</p>
<pre><code>
// 创建一个 GPUBuffer 对象，用于存储顶点数据
const vertexBuffer = device.createBuffer({
// 为了便于调试，可以为 GPUBuffer 对象添加一个标签
label: &#39;GPUBuffer store vertex&#39;,
// 指定 GPUBuffer 的大小，单位为字节，该属性应与存储在该缓冲区中的数据大小相匹配
size: triangle.vertex.byteLength,
// 指定 GPUBuffer 的用途，包括 VERTEX 和 COPY_DST 两种用途
// VERTEX 表示该缓冲区将存储顶点数据
// COPY_DST 表示该缓冲区可以作为 GPUBuffer.copyToBuffer() 方法的目标缓冲区
usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
// 如果希望在创建缓冲区后立即将数据写入该缓冲区，可以设置 mappedAtCreation 为 true
// 此时，将返回的缓冲区对象将包含一个 ArrayBuffer 对象，可以直接修改该 ArrayBuffer 的值，以修改缓冲区的内容
// 如果不设置该属性或者设置为 false，需要通过 GPUBuffer.mapAsync() 方法映射缓冲区才能修改缓冲区内容
// mappedAtCreation: true
})
</code></pre>
<pre><code>下面给上述WSGL代码添加注释：

// 索引为0的图形绑定，组0用于Shader中的I/O装饰符 
@binding(0) @group(0) var&lt;storage, read&gt; mvpMatrix : array&lt;mat4x4&lt;f32&gt;&gt;;

// 定义顶点着色程序的输出属性 
struct VertexOutput { // 系统内置变量，指定输出顶点的裁剪空间位置
    @builtin(position) Position : vec4&lt;f32&gt;, // 输出顶点的纹理坐标，位置在Shader程序中			@location(0)声明 
    @location(0) fragUV : vec2&lt;f32&gt;, // 输出顶点的位置，位置在Shader程序中
    @location(1) fragPosition: vec4&lt;f32&gt;
};

// 定义顶点着色程序
@vertex fn main( // 系统内置变量，获取实例的索引 
    @builtin(instance_index) index : u32, // 顶点位置坐标，位置在Shader程序中
    @location(0)声明 @location(0) position : vec4&lt;f32&gt;, // 顶点纹理坐标，位置在Shader程序中		@location(1)声明 @location(1) uv : vec2&lt;f32&gt; ) -&gt; VertexOutput { // 定义输出 
    var output : VertexOutput; // 计算输出位置矩阵，矩阵乘法通过*实现 
    output.Position = mvpMatrix[index] * position; // 输出顶点的纹理坐标 
    output.fragUV = uv; // 输出顶点的位置 
    output.fragPosition = 0.5 * (position + vec4&lt;f32&gt;(1.0, 1.0, 1.0, 1.0)); // 返回输出 return output; }
</code></pre>
<h1>模板</h1>
<pre><code class="language-ts">// initialize webgpu device &amp; config canvas context
async function initWebGPU(canvas: HTMLCanvasElement) {
    if(!navigator.gpu)
        throw new Error(&#39;Not Support WebGPU&#39;)
    const adapter = await navigator.gpu.requestAdapter({
        powerPreference: &#39;high-performance&#39;
        // powerPreference: &#39;low-power&#39;
    })
    if (!adapter)
        throw new Error(&#39;No Adapter Found&#39;)
    const device = await adapter.requestDevice()
    const context = canvas.getContext(&#39;webgpu&#39;) as GPUCanvasContext
    const format = navigator.gpu.getPreferredCanvasFormat()
    const devicePixelRatio = window.devicePixelRatio || 1
    canvas.width = canvas.clientWidth * devicePixelRatio
    canvas.height = canvas.clientHeight * devicePixelRatio
    const size = {width: canvas.width, height: canvas.height}
    context.configure({
        // json specific format when key and value are the same
        device, format,
        // prevent chrome warning
        alphaMode: &#39;opaque&#39;
    })
    return {device, context, format, size}
}
// create a simple pipiline
async function initPipeline(device: GPUDevice, format: GPUTextureFormat): Promise&lt;GPURenderPipeline&gt; {
    const descriptor: GPURenderPipelineDescriptor = {
        layout: &#39;auto&#39;,   // 待编辑
        vertex: {
            module: device.createShaderModule({
                code: triangleVert   // 待编辑
            }),
            entryPoint: &#39;main&#39;
        },
        primitive: {
            topology: &#39;triangle-list&#39; // try point-list, line-list, line-strip, triangle-strip?
        },
        fragment: {
            module: device.createShaderModule({
                code: redFrag   // 待编辑
            }),
            entryPoint: &#39;main&#39;,
            targets: [
                {
                    format: format
                }
            ]
        }
    }
    return await device.createRenderPipelineAsync(descriptor)
}
// create &amp; submit device commands
function draw(device: GPUDevice, context: GPUCanvasContext, pipeline: GPURenderPipeline) {
    const commandEncoder = device.createCommandEncoder()
    const view = context.getCurrentTexture().createView()
    const renderPassDescriptor: GPURenderPassDescriptor = {
        colorAttachments: [
            {
                view: view,
                clearValue: { r: 0, g: 0, b: 0, a: 1.0 },
                loadOp: &#39;clear&#39;, // clear/load
                storeOp: &#39;store&#39; // store/discard
            }
        ]
    }
    const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor)
    passEncoder.setPipeline(pipeline)
    // 3 vertex form a triangle
    passEncoder.draw(3)   // 待编辑
    passEncoder.end()
    // webgpu run in a separate process, all the commands will be executed after submit
    device.queue.submit([commandEncoder.finish()])
}

async function run(){
    const canvas = document.querySelector(&#39;canvas&#39;)
    if (!canvas)
        throw new Error(&#39;No Canvas&#39;)
    const {device, context, format} = await initWebGPU(canvas)
    const pipeline = await initPipeline(device, format)
    // start draw
    draw(device, context, pipeline)
    
    // re-configure context on resize
    window.addEventListener(&#39;resize&#39;, ()=&gt;{
        canvas.width = canvas.clientWidth * devicePixelRatio
        canvas.height = canvas.clientHeight * devicePixelRatio
        // don&#39;t need to recall context.configure() after v104
        draw(device, context, pipeline)
    })
}
run()
</code></pre>
<h1>GPT</h1>
<p>@vertex fn main(@location(0) pos: vec3<f32>) -&gt; @builtin(position) vec4<f32>{    return vec4<f32>(pos,1.0);//返回顶点数据，渲染管线下个环节使用 }是什么意思，@builtin(position) vec4<f32>是什么意思，@location(0) pos: vec3<f32>是什么意思</p>
<p>这是一个使用WGSL(WebGPU Shading Language)编写的计算着色器，主要用于顶点（vertex）处理。</p>
<ul>
<li><pre><code>fn main(@location(0) pos: vec3&lt;f32&gt;) -&gt; @builtin(position) vec4&lt;f32&gt;
</code></pre>
<p>：</p>
<ul>
<li><code>fn main()</code> 是 WGSL 着色器函数的入口点。</li>
<li><code>@location(0) pos:</code> 表示顶点属性 pos 的存储位置在 0 号缓冲区。顶点属性是指每个顶点自己持有的数据信息（如顶点位置、法线方向等），可以由用户传人 GPU 内存中进行处理和计算。</li>
<li><code>vec3&lt;f32&gt;</code> 表示 pos 类型为 <code>vec3</code>，其中 <code>f32</code> 表示数据类型为 32 位浮点数。</li>
<li><code>-&gt;@builtin(position) vec4&lt;f32&gt;</code>：箭头后的语句指着色器的返回值类型，即输出结果。<code>@builtin(position)</code> 是预定义的装饰符，用来指示该变量是内置变量（即该位置已经被显式地保留给 GPU 渲染管线的某个阶段使用），在此处它表示输出变量的位置信息。<code>vec4&lt;f32&gt;</code> 则表示此处返回的是四维向量，其中每个分量都是 32 位浮点数，代表了顶点在裁剪空间中的位置坐标和透视信息。</li>
</ul>
</li>
</ul>
<p>因此，以上代码的作用是接收顶点的位置信息，并将其转换为裁剪空间中的坐标位置输出给渲染管线的下一个阶段。</p>
<h2>window.devicePixelRatio是什么</h2>
<p><code>window.devicePixelRatio</code> 是一个JavaScript属性，它返回当前显示器上物理像素和CSS像素之间的比率。</p>
<p>当在高分辨率设备（如Retina屏幕）上显示网页时，每个CSS像素通常映射到多个物理像素。例如，在Retina屏幕上，一个CSS像素可能对应四个物理像素，因此网站需要适当地缩放图像和文字来避免在高分辨率设备上显示不清晰或过大。<code>window.devicePixelRatio</code> 可以用于检测这种情况，并根据需要进行适当的调整。</p>
<p><code>window.devicePixelRatio</code> 的值等于物理像素数除以CSS像素数。例如，如果物理像素的数量是CSS像素的两倍，则 <code>window.devicePixelRatio</code> 将为2。</p>
<h2>html的canvas的getContext方法</h2>
<p>在 HTML 中，Canvas 是一种可编程的二维绘图区域，我们可以使用 Canvas API 在其中绘制各种图形、文本以及图像。</p>
<p>要开始绘制，我们需要获取画布上下文（context）。<code>&lt;canvas&gt;</code>元素提供了一个方法 <code>getContext()</code> 用于获得画布上下文。该方法有一个参数，用于指定上下文环境类型，例如：</p>
<ul>
<li><code>2d</code>：表示二维渲染上下文，通过该上下文，我们可以在 Canvas 上绘制各种图形，如线条、矩形、圆形、文本等。</li>
<li><code>webgl</code> 或 <code>experimental-webgl</code>：表示 WebGL 上下文，它是一种基于 OpenGL ES2.0 的 JavaScript API，用于高性能的 3D 绘图。</li>
<li><code>webgl2</code>：表示 WebGL2 上下文，在 WebGL 基础上增加了更多功能和特性。</li>
</ul>
<p>以下是获取二维上下文的示例：</p>
<pre><code>html复制代码&lt;canvas id=&quot;myCanvas&quot;&gt;&lt;/canvas&gt;

&lt;script&gt;
    const canvas = document.getElementById(&#39;myCanvas&#39;);
    const ctx = canvas.getContext(&#39;2d&#39;);

    // 在画布上绘制绿色填充的正方形
    ctx.fillStyle = &#39;green&#39;;
    ctx.fillRect(10, 10, 50, 50);
&lt;/script&gt;
</code></pre>
<p>在这个例子中，我们首先获取了 <code>id</code> 为 <code>myCanvas</code> 的 <code>&lt;canvas&gt;</code> 元素，并在 JavaScript 中将其存储在变量 <code>canvas</code> 中。接着，我们调用了 <code>canvas</code> 对象的 <code>getContext()</code> 方法，传入参数 <code>&#39;2d&#39;</code>，获取了一个二维上下文，将其存储在变量 <code>ctx</code> 中。最后，我们使用上下文对象 <code>ctx</code> 绘制了一个绿色填充的正方形。</p>
<h2>device.createBindGroup()</h2>
<p><code>device.createBindGroup()</code>是WebGPU API中的一种方法，用于创建绑定组对象（Binding Group Object）。</p>
<p>绑定组是一组在渲染管线中使用的资源。它们包含着一个或多个已绑定着资源视图（Resource View），而这些资源视图表示了纹理、缓冲区、采样器以及其他WebGPU中可用的资源类型。创建和管理绑定组通常是通过私有数据结构完成的。</p>
<p>调用<code>device.createBindGroup()</code>时需要传入一个描述符对象作为参数，描述符对象至少应当包括以下信息：</p>
<ul>
<li>layout: 与创建当前绑定组相关联的布局对象</li>
<li>bindings: 包含资源绑定信息的数组</li>
<li>label（非必需）：绑定组的名称，用于调试目的</li>
</ul>
<p>其中，layout是指该绑定组的布局描述符(layout descriptor)，用于确定绑定组中各项资源视图的类型、大小、偏移等信息；bindings是一个包含许多绑定的数组，每个绑定描述了将资源绑定到渲染管线中的特定槽口的方式。</p>
<p>示例代码：</p>
<pre><code>js复制代码const bindGroupLayoutDesc = {
    bindings: [
        {
            binding: 0,           // 第一个 binding point 的索引，本例中为 0 
            visibility: GPUShaderStage.VERTEX,  // 使用的着色器阶段
            buffer: {             // 绑定的具体资源视图
                type: &quot;uniform&quot;
            }
        },
        ...
    ]
}
const bindGroupLayout = device.createBindGroupLayout(bindGroupLayoutDesc);

// 创建 descriptors 列表中的第一个绑定点
const buffer = device.createBuffer({
    size: 1024,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.MAP_WRITE //可以被渲染管线访问和写入
});

const bindGroupDescriptor = {
    layout: bindGroupLayout,  // 绑定组关联的布局对象
    bindings
</code></pre>
<h2>webgpu的GPUBindGroupEntry</h2>
<p><code>GPUBindGroupEntry</code> 是 WebGPU API 中的一种数据类型，它表示 GPU 绑定组（Bind Group）中的一个条目。在WebGPU程序中，GPU绑定组由多个条目组成，每个条目都包含绑定资源的信息。</p>
<p><code>GPUBindGroupEntry</code> 包含以下属性：</p>
<ul>
<li><code>binding</code>: 表示该条目在 GPU 绑定组中的位置，通常对应于着色器代码中的绑定点。</li>
<li><code>resource</code>: 表示该条目所关联的绑定资源。这可以是缓冲区、纹理、采样器或者其他的 GPU 对象，取决于绑定点所要求的类型。</li>
</ul>
<p><code>GPUBindGroupEntry</code> 可以用来创建新的 GPU 绑定组，在传递命令给 GPU 前，必须先将资源绑定到合适的绑定点上。使用 <code>GPUBindGroupEntry</code> 可以为每个绑定点指定要使用的特定资源，从而告诉 GPU 如何执行渲染或计算任务。</p>
<p> 详细说明一下：const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor);    passEncoder.setPipeline(pipelineObj.pipeline);    // set uniformGroup    passEncoder.setBindGroup(0, pipelineObj.uniformGroup)    // set vertex    passEncoder.setVertexBuffer(0, pipelineObj.vertexBuffer)    // 3 vertex form a triangle    passEncoder.draw(triangle.vertexCount);    passEncoder.end()    device.queue.submit([commandEncoder.finish()])</p>
<p>这段代码使用WebGPU API对渲染管道进行了一些设置并提交给GPU进行渲染。</p>
<ul>
<li><p><code>const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor)</code>：创建一个用于执行渲染命令的<code>passEncoder</code>对象。通过调用<code>beginRenderPass()</code>方法并传入描述渲染过程的<code>renderPassDescriptor</code>参数。</p>
</li>
<li><p><code>passEncoder.setPipeline(pipelineObj.pipeline)</code>：设置绑定当前渲染中的着色器管道，其包含了渲染所需的命令和状态信息。</p>
</li>
<li><pre><code>passEncoder.setBindGroup(0, pipelineObj.uniformGroup)
</code></pre>
<p>：将Uniform数据绑定到指定的Bind Group。</p>
<ul>
<li>Uniform数据是渲染中的常量值，可以被所有的着色器访问，并用于控制图形的外观。</li>
<li>Bind Group表示存储着多个绑定的资源，比如Uniform变量、纹理数据等。</li>
<li><code>pipelineObj.uniformGroup</code>是一个包含渲染所需Uniform变量数据的Bind Group对象。</li>
</ul>
</li>
<li><pre><code>passEncoder.setVertexBuffer(0, pipelineObj.vertexBuffer)
</code></pre>
<p>：设置用于渲染的顶点缓冲区。</p>
<ul>
<li>Vertex Buffer存储顶点的位置和属性数据，在渲染过程中会用到。</li>
<li><code>pipelineObj.vertexBuffer</code>是一个包含渲染所需顶点数据的Buffer对象。</li>
</ul>
</li>
<li><pre><code>passEncoder.draw(triangle.vertexCount)
</code></pre>
<p>：利用现有状态，绘制几何体（比如三角形）。</p>
<ul>
<li><code>triangle.vertexCount</code>指定绘制三角形需要使用的顶点数量。</li>
</ul>
</li>
<li><p><code>passEncoder.end()</code>：结束渲染命令的记录</p>
</li>
<li><p><code>device.queue.submit([commandEncoder.finish()])</code>：提交已经记录好的所有渲染命令到设备进行执行。</p>
</li>
</ul>
<pre><code>这是一段wgsl着色器的代码，帮我详细说明一下它的工作流程：
@binding(0) @group(0) var&lt;storage, read&gt; mvpMatrix : array&lt;mat4x4&lt;f32&gt;&gt;;

struct VertexOutput {
    @builtin(position) Position : vec4&lt;f32&gt;,
    @location(0) fragUV : vec2&lt;f32&gt;,
    @location(1) fragPosition: vec4&lt;f32&gt;

};

@vertex
fn main(
    @builtin(instance_index) index : u32,
    @location(0) position : vec4&lt;f32&gt;,
    @location(1) uv : vec2&lt;f32&gt;
) -&gt; VertexOutput {
    var output : VertexOutput;
    output.Position = mvpMatrix[index] * position;
    output.fragUV = uv;
    output.fragPosition = 0.5 * (position + vec4&lt;f32&gt;(1.0, 1.0, 1.0, 1.0));
    return output;
}
这段代码是一个WGSL（WebGPU Shading Language）着色器，它定义了一个顶点着色器函数。让我详细说明一下它的工作流程：

1. 引入了一个存储绑定（storage binding）和一个组（group）绑定，用于绑定一个矩阵数组 `mvpMatrix` 到绑定点 0。
2. 定义了一个名为 `VertexOutput` 的结构体，它包含了三个成员变量：
   - `Position`：使用内置修饰符 `@builtin(position)` 标记为顶点位置（裁剪空间坐标）。
   - `fragUV`：使用修饰符 `@location(0)` 标记为片段着色器的纹理坐标输入。
   - `fragPosition`：使用修饰符 `@location(1)` 标记为片段着色器的位置（世界空间坐标）输入。
3. 定义了一个顶点着色器函数 `main`，接受三个输入参数：`index`、`position` 和 `uv`。
   - `index` 是内置变量 `@builtin(instance_index)`，表示当前实例的索引。
   - `position` 是使用修饰符 `@location(0)` 标记的顶点位置输入参数。
   - `uv` 是使用修饰符 `@location(1)` 标记的纹理坐标输入参数。
4. 在函数体内部，声明了一个名为 `output` 的局部变量，类型为 `VertexOutput` 结构体。
5. 计算顶点位置的变换结果：`output.Position = mvpMatrix[index] * position;`。这里使用了实例索引 `index` 来索引 `mvpMatrix` 数组，并将顶点位置 `position` 与矩阵相乘，得到变换后的顶点位置。
6. 将输入的纹理坐标赋值给输出的 `fragUV` 成员：`output.fragUV = uv;`。
7. 计算片段位置（世界空间坐标）的值：`output.fragPosition = 0.5 * (position + vec4&lt;f32&gt;(1.0, 1.0, 1.0, 1.0));`。这里将顶点位置 `position` 与一个固定的向量相加，并将结果乘以 0.5，得到片段位置。
8. 返回输出的 `output` 结构体作为函数的结果。

总的来说，这个顶点着色器函数接收顶点位置和纹理坐标作为输入，在进行一些变换计算后，输出了变换后的顶点位置、纹理坐标以及片段位置信息。这些输出将用于后续的渲染管线阶段，例如光栅化和片段着色器。
</code></pre>
<pre><code>在webgpu中，这两段代码是什么意思：
    primitive: {
      topology: &quot;triangle-list&quot;,
      cullMode: &quot;back&quot;,
    },
    depthStencil: {
      depthWriteEnabled: true,
      depthCompare: &quot;less&quot;,
      format: &quot;depth24plus&quot;,
    },
</code></pre>
<p>在WebGPU中，这两段代码分别定义了渲染管线的基本几何形状（primitive）设置和深度/模板测试设置。</p>
<p>第一段代码：</p>
<pre><code class="language-javascript">primitive: {
  topology: &quot;triangle-list&quot;,
  cullMode: &quot;back&quot;,
},
</code></pre>
<p>这段代码指定了基本几何形状的设置。具体含义如下：</p>
<ul>
<li><code>topology: &quot;triangle-list&quot;</code>：指定基本几何形状为三角形列表。渲染管线将使用传递给它的顶点数据来构建三角形。</li>
<li><code>cullMode: &quot;back&quot;</code>：指定背面剔除模式为剔除背面。在渲染过程中，背面朝向相机的三角形将被剔除，不会被渲染。</li>
</ul>
<p>第二段代码：</p>
<pre><code class="language-javascript">depthStencil: {
  depthWriteEnabled: true,
  depthCompare: &quot;less&quot;,
  format: &quot;depth24plus&quot;,
},
</code></pre>
<p>这段代码指定了深度/模板测试的设置。具体含义如下：</p>
<ul>
<li><code>depthWriteEnabled: true</code>：启用深度写入。在渲染过程中，深度值将根据像素的位置进行更新，从而实现深度测试。</li>
<li><code>depthCompare: &quot;less&quot;</code>：指定深度比较函数为&quot;less&quot;。深度测试将使用传入的像素深度值与深度缓冲区中的现有深度值进行比较，只有当传入的像素深度值小于现有深度值时才会通过测试。</li>
<li><code>format: &quot;depth24plus&quot;</code>：指定深度缓冲区的格式为&quot;depth24plus&quot;。这表示深度缓冲区将使用24位深度值，并且可能还包括其他附加的深度信息（例如模板）。</li>
</ul>
<p>综合起来，这两段代码定义了渲染管线的基本几何形状和深度/模板测试的设置。在渲染过程中，根据指定的基本几何形状和深度/模板测试设置，WebGPU将根据顶点数据生成三角形，并使用深度测试来确定像素是否应该被渲染，以及是否剔除背面三角形。</p>
<p>当渲染器执行顶点着色器时，它会为每个输入顶点调用顶点着色器函数。在您提供的顶点着色器中，以下是其大致的工作流程：</p>
<ol>
<li>顶点着色器函数 <code>main()</code> 被调用，接受输入参数 <code>index</code>、<code>position</code> 和 <code>uv</code>。</li>
<li>创建一个 <code>VertexOutput</code> 结构体实例 <code>output</code>，用于存储顶点着色器的输出数据。</li>
<li>计算变换后的顶点位置：<ul>
<li>通过 <code>mvpMatrix[index]</code> 获取与当前顶点关联的模型-视图-投影矩阵。</li>
<li>将该矩阵与输入顶点位置 <code>position</code> 相乘，得到变换后的位置。</li>
<li>将结果存储在 <code>output.Position</code> 中。</li>
</ul>
</li>
<li>将输入的纹理坐标 <code>uv</code> 存储在 <code>output.fragUV</code> 中，用于传递给片元着色器。</li>
<li>计算一个位于 [-0.5, 0.5] 范围内的偏移位置 <code>fragPosition</code>：<ul>
<li>通过将 <code>position</code> 与 <code>(1.0, 1.0, 1.0, 1.0)</code> 相加，得到一个位于 [0, 2] 范围内的位置。</li>
<li>将结果除以 2，将位置范围映射到 [-0.5, 0.5]。</li>
<li>将结果存储在 <code>output.fragPosition</code> 中。
（注释中的另一行 <code>// output.fragPosition = position;</code> 被注释掉了，因此当前不会执行）</li>
</ul>
</li>
<li>将 <code>output</code> 返回作为顶点着色器的输出。</li>
</ol>
<p>顶点着色器的主要任务是将输入顶点的位置进行变换，并生成输出数据供片元着色器使用。在这个特定的顶点着色器中，模型-视图-投影矩阵被用于将对象从模型空间转换到剪裁空间，纹理坐标和偏移位置被传递给片元着色器进行进一步处理。</p>
<p>请注意，顶点着色器在渲染管线的顶点处理阶段执行，每个顶点都会经过顶点着色器函数，并将输出传递给下一个阶段，即片元着色器。</p>
<p>在您提供的顶点着色器代码中，顶点着色器的输入是以下几个参数：</p>
<ol>
<li><code>index</code>：<code>@builtin(instance_index)</code> 注解指示此参数为实例索引，类型为 <code>u32</code>，表示当前顶点所属的实例索引。</li>
<li><code>position</code>：<code>@location(0)</code> 注解指示此参数为位置属性，类型为 <code>vec4&lt;f32&gt;</code>，表示当前顶点的位置坐标。</li>
<li><code>uv</code>：<code>@location(1)</code> 注解指示此参数为纹理坐标属性，类型为 <code>vec2&lt;f32&gt;</code>，表示当前顶点的纹理坐标。</li>
</ol>
<p>您提到的 <code>mvpBuffer</code> 在顶点着色器中没有直接出现。然而，在顶点着色器的主函数 <code>main()</code> 中，通过索引 <code>index</code>，使用 <code>mvpMatrix</code> 数组进行矩阵乘法运算，计算变换后的顶点位置。</p>
<pre><code class="language-glsl">output.Position = mvpMatrix[index] * position;
</code></pre>
<p>这里假设 <code>mvpMatrix</code> 是一个存储了模型-视图-投影矩阵的数组，并且使用 <code>index</code> 来索引当前顶点所关联的矩阵。这样，顶点的位置会被变换为剪裁空间中的位置。</p>
<p>因此，<code>mvpMatrix</code> 可以被视为顶点着色器的外部输入，但在顶点着色器的函数参数中并没有直接使用 <code>mvpBuffer</code> 这个名称。在代码中，<code>mvpMatrix</code> 是通过 <code>@binding</code> 和 <code>@group</code> 注解修饰的变量，它被指定为存储限定符为 <code>storage</code>，读取限定符为 <code>read</code>，表示从外部存储中读取数据。</p>
<p><strong>@binding(0) @group(0) var&lt;storage, read&gt; mvpMatrix : array&lt;mat4x4<f32>&gt;;这一行代码具体解释一下</strong></p>
<p>这一行代码使用了一些属性修饰符和存储修饰符来声明变量 <code>mvpMatrix</code>，并指定了它的类型和绑定信息。让我们逐个解释这些修饰符的含义：</p>
<ul>
<li><p><code>@binding(0)</code>: 这是一个属性修饰符，用于指定变量的绑定点。在着色器中，变量需要绑定到特定的资源，例如缓冲区或纹理。绑定点是资源在着色器中的索引或标识符。这里的 <code>0</code> 表示绑定点的值为 0，即该变量与绑定点 0 相关联。</p>
</li>
<li><p><code>@group(0)</code>: 这也是一个属性修饰符，用于指定变量所属的组。在一些图形 API 中，资源可以被组织为逻辑上的组，以提高数据传输的效率。这里的 <code>0</code> 表示该变量属于组 0。</p>
</li>
<li><p><code>var&lt;storage, read&gt;</code>: 这是存储修饰符，用于指定变量的存储属性。<code>storage</code> 表示变量将存储在外部存储中，而不是寄存器中。<code>read</code> 表示变量将以只读方式访问。</p>
</li>
<li><p><code>mvpMatrix</code>: 这是变量的名称，您可以根据需要自由选择变量的名称。</p>
</li>
<li><p><code>: array&lt;mat4x4&lt;f32&gt;&gt;</code>: 这是变量的类型声明。在这里，<code>mvpMatrix</code> 被声明为一个 <code>mat4x4&lt;f32&gt;</code> 类型的数组。<code>mat4x4&lt;f32&gt;</code> 表示一个 4x4 的浮点数矩阵，用于存储模型-视图-投影矩阵。</p>
</li>
</ul>
<p>综上所述，该行代码声明了一个名为 <code>mvpMatrix</code> 的变量，它是一个存储在外部存储中的只读 <code>mat4x4&lt;f32&gt;</code> 类型的数组。该变量与绑定点 0 相关联，属于组 0。这意味着在使用这个顶点着色器之前，您需要将实际的模型-视图-投影矩阵数据绑定到绑定点 0 上，以便顶点着色器可以访问和使用这些数据。</p>
<h2>wgpu-matrix&amp;&amp;gl-matrix</h2>
<p>&quot;wgpu-matrix&quot; 和 &quot;gl-matrix&quot; 是两个不同的矩阵操作库，针对不同的图形编程环境进行了优化。</p>
<ol>
<li><p>WebGPU vs WebGL: &quot;wgpu-matrix&quot; 是专为 WebGPU（Web Graphics API）设计的库，而 &quot;gl-matrix&quot; 则是为 WebGL（Web Graphics Library）设计的库。WebGL 是基于 OpenGL ES 的图形库，用于在浏览器中进行 3D 图形渲染，而 WebGPU 是一个新的、更现代的图形 API，旨在提供更高效和更灵活的图形编程能力。</p>
</li>
<li><p>底层实现: &quot;wgpu-matrix&quot; 使用基于 WebGPU 的底层操作，利用了现代 GPU 的并行计算能力，以实现更高性能和效率。而 &quot;gl-matrix&quot; 则是在 WebGL 环境中使用 JavaScript 进行矩阵计算，其底层实现是针对 WebGL 的特性进行优化。</p>
</li>
<li><p>功能和接口: 两个库在功能和接口上也存在一些差异。 &quot;wgpu-matrix&quot; 主要专注于提供与 WebGPU 相关的矩阵操作，例如在 GPU 上进行矩阵变换和计算。它可能提供一些特定于 WebGPU 的功能和优化，以利用底层图形编程 API 的特性。而 &quot;gl-matrix&quot; 则是一个功能更为全面的矩阵库，提供了广泛的矩阵计算功能和转换操作，适用于 WebGL 环境中的各种图形编程需求。</p>
</li>
</ol>
<p>总之，&quot;wgpu-matrix&quot; 和 &quot;gl-matrix&quot; 是面向不同图形编程环境的矩阵操作库。选择哪个库取决于你的图形编程环境和需求。如果你正在使用 WebGPU 进行图形编程，那么 &quot;wgpu-matrix&quot; 可能是一个更适合的选择，因为它专为 WebGPU 进行了优化。而如果你在使用 WebGL 进行图形编程，那么 &quot;gl-matrix&quot; 可能是更常用和广泛支持的选择，因为它是为 WebGL 环境设计的成熟库。</p>
<h2>renderBundle</h2>
<p>orillsion的renderBundle例子</p>
<p>这段代码使用了WebGPU来实现渲染和资源绑定，并利用<code>renderBundle</code>来提高性能。下面是代码中使用<code>renderBundle</code>实现物体资源绑定和渲染的流程：</p>
<ol>
<li><p>初始化WebGPU：在<code>initWebGPU</code>函数中，首先初始化WebGPU设备和Canvas上下文，并设置渲染表面的格式和大小。</p>
</li>
<li><p>创建渲染管线和缓冲：在<code>initPipeline</code>函数中，创建了渲染管线，并初始化了顶点缓冲和存储缓冲。还创建了用于存储矩阵的Uniform Buffer，并创建了一个包含矩阵的绑定组。</p>
</li>
<li><p>创建并记录<code>renderBundle</code>：在<code>run</code>函数中，通过循环遍历场景中的对象，设置渲染管线的顶点缓冲和绑定组，并调用<code>draw</code>命令来绘制每个对象。这样可以记录<code>renderBundle</code>，以便在后续的渲染中重复使用。</p>
</li>
<li><p>渲染循环：在<code>frame</code>函数中，更新场景中每个对象的旋转并更新存储缓冲的数据。然后，创建渲染通道描述符，并调用<code>draw</code>函数来执行渲染命令，同时传递之前记录好的<code>renderBundle</code>。这样可以实现物体的渲染。</p>
</li>
<li><p>调整大小：在窗口大小调整时，重新配置Canvas和深度纹理，并更新渲染通道描述符中的视口。</p>
</li>
</ol>
<p>通过使用<code>renderBundle</code>，可以将渲染命令和资源绑定预先记录下来，并在渲染循环中重复使用，以提高性能并减少CPU开销。这样，在每次渲染时，只需更新存储缓冲的数据并执行<code>draw</code>命令，而无需重复设置渲染管线和资源绑定。</p>
<p>需要注意的是，<code>renderBundle</code>的使用要求浏览器和设备支持WebGPU，并且在某些浏览器版本中可能是实验性的功能。确保在目标平台上正确支持<code>renderBundle</code>功能。</p>
<h1>常用词汇</h1>
<p>统一缓冲区：<a href="https://www.orillusion.com/zh/wgsl.html#uniform-buffer">uniform buffer</a></p>
<p>存储缓冲区： <a href="https://www.orillusion.com/zh/wgsl.html#storage-buffer">storage buffer</a></p>
<p>光栅化：rasterization</p>
<p>光照：illumination</p>
<p>photon: 光子</p>
<p>bounce: 反弹</p>
<p>存储： Storage</p>
<p>Uniform：Uniform变量在整个渲染过程中始终保持相同的值，因此它们被称为均匀量或统一量</p>
<p>mesh： 网格</p>
<p>块：block</p>
<p>Negative：负的  <strong>背面？</strong></p>
<p>emissive：发射的</p>
<p>visibility: 可见性   // visibility: GPUShaderStage.VERTEX | GPUShaderStage.COMPUTE,</p>
<p>rasterizer: 光栅器</p>
<p>Radiosity: 光能传递</p>
<p>invocation: 调用</p>
<p>Accumulation: 积累</p>
<p>f32（32位浮点数字）
i32（32位整数）
u32（32位无符号整数）
f16（16位浮点数字）</p>
<h1>Games101</h1>
<p>旋转矩阵不好求，但是它的逆好求，旋转矩阵就是正交矩阵，它的逆就是它的转置，</p>
<h2>法线经过mvp变换求法</h2>
<pre><code>在计算机图形学中，对于经过变换后的顶点，法向量的计算通常需要进行额外的处理。下面是一种常见的方法：

1. 首先，你需要有顶点的原始法向量。通常情况下，这些法向量可以在模型创建或导入的过程中获得。

2. 在进行顶点变换之前，你需要对法向量进行一些修正。一般来说，顶点的法向量需要经过法线矩阵的逆转置来进行变换，以保持正确的方向和长度。法线矩阵（通常是模型视图矩阵的逆转置）可以用来处理法向量的变换。假设法线矩阵为M，并且顶点的原始法向量为V，变换后的法向量N可以通过以下方式计算得到：

   N = (M^-1)^T * V

   其中 (M^-1)^T 表示法线矩阵的逆转置。

3. 接下来，在将顶点坐标乘以 MVP（模型视图投影）矩阵的过程中，对法向量进行类似的变换。你可以使用法线矩阵的逆转置来变换法向量。即对于经过 MVP 变换后的顶点的法向量，可以通过以下方式计算得到：

   N&#39; = (MVP^-1)^T * V&#39;

   其中 N&#39; 是变换后的法向量，MVP^-1 表示 MVP 矩阵的逆转置，V&#39; 是变换后的顶点坐标。

这样，你就可以获得经过变换后的顶点的法向量。这个法向量可以用于光照计算、阴影计算等图形渲染的过程中。需要注意的是，这种方法假设了变换矩阵没有包含非均匀缩放的部分，且没有使用投影矩阵的透视除法。在一些特殊情况下，可能需要使用其他方法来处理法向量的变换。
</code></pre>
<h2>光栅化</h2>
<p>判断点和三角形的内外关系</p>
<p><img src="webgpu.assets/image-20230615151826919.png" alt="image-20230615151826919"></p>
<p>频率和采样频率</p>
<p><img src="webgpu.assets/image-20230615154255288.png" alt="image-20230615154255288"></p>
<p>走样：一种采样方法在两种截然不同频率的函数上得到相似的采样结果</p>
<p><img src="webgpu.assets/image-20230615154328905.png" alt="image-20230615154328905"></p>
<p>一张图片可以在时域和频域中进行分析。</p>
<ol>
<li><strong>时域</strong>：在时域中，一张图片可以看作是一个二维的信号，其中横轴和纵轴分别表示像素的位置。在时域中，我们可以观察到图像中每个像素随着位置的变化，从而了解图像的空间分布、颜色变化等特性。</li>
<li><strong>频域</strong>：将图像转换到频域可以通过二维傅里叶变换实现。在频域中，图像的表示以频率分量为主，可以分析图像中包含的不同空间频率成分。频域中的低频分量通常对应于图像中的平滑区域，而高频分量对应于图像中的细节或边缘。</li>
</ol>
<p>频域分析对于图像处理中的一些操作（例如滤波、压缩等）非常有用，因为它允许我们以频率的角度来理解图像的特性。</p>
<p>因此，时域和频域分析在图像处理中都是非常重要的工具，它们提供了不同的视角来理解和处理图像。</p>
<p>滤波：过滤掉特定频率信息</p>
<p>中心往外：低频到高频</p>
<p><img src="webgpu.assets/image-20230615154556667.png" alt="image-20230615154556667"></p>
<p>高频：边界</p>
<p>时域卷积=频域乘积</p>
<p><img src="webgpu.assets/image-20230615155359559.png" alt="image-20230615155359559"></p>
<p>越大的box卷积越模糊</p>
<p><img src="webgpu.assets/image-20230615155630156.png" alt="image-20230615155630156"></p>
<p><img src="webgpu.assets/image-20230615160029727.png" alt="image-20230615160029727"></p>
<p>采样就是对频谱复制粘贴</p>
<p>走样：</p>
<p><img src="webgpu.assets/image-20230615160013326.png" alt="image-20230615160013326"></p>
<p>先模糊，去除高频信号，在采样</p>
<p><img src="webgpu.assets/image-20230615160238960.png" alt="image-20230615160238960"></p>
<p>MSAA
更多采样点，近似得到合理的覆盖率</p>
<p><img src="webgpu.assets/image-20230615160732017.png" alt="image-20230615160732017"></p>
<p><img src="webgpu.assets/image-20230615161134351.png" alt="image-20230615161134351"></p>
<h2>Blinn-Phong Reflectance Model</h2>
<p>着色不考虑阴影</p>
<p><img src="webgpu.assets/image-20230615164703035.png" alt="image-20230615164703035"></p>
<p><strong>漫反射</strong></p>
<p>Kd:漫反射吸收系数  PS:漫反射吸收系数是指材料对入射光进行漫反射时，部分光能被材料吸收的比例</p>
<p>点乘：夹角余弦</p>
<p><img src="webgpu.assets/image-20230615165535960.png" alt="image-20230615165535960"></p>
<p><strong>镜面反射</strong></p>
<p>h：半程向量；是否可以看到高光，可以用n和h的夹角余弦间接反应</p>
<p><img src="webgpu.assets/image-20230615182816748.png" alt="image-20230615182816748"></p>
<p><strong>环境光</strong></p>
<p><img src="webgpu.assets/image-20230615183334155.png" alt="image-20230615183334155"></p>
<p><img src="webgpu.assets/image-20230615183413427.png" alt="image-20230615183413427"></p>
<p><strong>面着色 顶点着色 像素着色</strong></p>
<p>顶点法线：关联平面法线加权平均</p>
<p>像素法线：使用重心坐标</p>
<p><img src="webgpu.assets/image-20230615184213013.png" alt="image-20230615184213013"></p>
<h2>实时渲染管线</h2>
<p><img src="webgpu.assets/image-20230615184352712.png" alt="image-20230615184352712"></p>
<h2>纹理映射</h2>
<h2>重心坐标</h2>
<p>三维中，先差值再投影到二维</p>
<p><img src="webgpu.assets/image-20230615195422592.png" alt="image-20230615195422592"></p>
<p>使用重心坐标差值三角形内每个像素的属性
<img src="webgpu.assets/image-20230615195510872.png" alt="image-20230615195510872"></p>
<p>如何纹理映射：</p>
<p>重心坐标-&gt;像素对应纹理的uv坐标-&gt;着色(uv对应颜色替换漫反射系数)</p>
<p><img src="webgpu.assets/image-20230615195714282.png" alt="image-20230615195714282"></p>
<p>纹理放大&amp;缩小    单像素覆盖纹理大小影响</p>
<p><img src="webgpu.assets/image-20230615203641970.png" alt="image-20230615203641970"></p>
<p>Mipmap： 范围查询</p>
<p><img src="webgpu.assets/image-20230615203712823.png" alt="image-20230615203712823"></p>
<p>L=1即原图，=4可看做放大4倍，直接查询第2层</p>
<p><img src="webgpu.assets/image-20230615204325046.png" alt="image-20230615204325046"></p>
<p>如果1.8层：(三线性插值)</p>
<p>一层二层分别差值，再两层之间在差值</p>
<p><img src="webgpu.assets/image-20230615204628840.png" alt="image-20230615204628840"></p>
<p>Anisotropic Filtering：各向异性过滤(解决纹理垂直水平拉升)</p>
<p><img src="webgpu.assets/image-20230615205124283.png" alt="image-20230615205124283"></p>
<p><img src="webgpu.assets/image-20230615205002400.png" alt="image-20230615205002400"></p>
<p>纹理：数据 可范围查询 作为环境光 凹凸贴图等等</p>
<p>凹凸贴图</p>
<p>求切线，切线旋转90度得到新的法线，再计算着色</p>
<img src="webgpu.assets/image-20230615210857214.png" alt="image-20230615210857214" style="zoom:150%;" />

<p>位移贴图</p>
<h2>几何</h2>
<p>隐式&amp;显式   距离函数</p>
<p>贝塞尔曲线  贝塞尔曲面</p>
<h2>网格细分</h2>
<p><strong>Catmull-Clark Subdivision (General Mesh)</strong></p>
<p>一次细分后，奇异点增加，但是消除了三角形</p>
<p><img src="webgpu.assets/image-20230616105446012.png" alt="image-20230616105446012"></p>
<p>网格简化</p>
<p>Collapsing An Edge:	点坍缩</p>
<p>Quadric Error Metrics：⼆次误差度量</p>
<p>shader map</p>
<p>精度问题</p>
<p><img src="webgpu.assets/image-20230616111626449.png" alt="image-20230616111626449"></p>
<h2>Ray tracing</h2>
<h3>(Whitted-Style) Ray Tracing</h3>
<p><strong>不合理的地方</strong>：</p>
<p><img src="webgpu.assets/image-20230616203916429.png" alt="image-20230616203916429"></p>
<p><strong>1 光线一定沿着直线传播</strong>
<strong>2 光线之间无法碰撞</strong>
<strong>3 光线路径可逆，即从A发出的到B的光线，一定也可以从B发出到A（中途可发生反射和折射）</strong></p>
<p><strong>每一个</strong>交点的颜色贡献来自这样种几类型 <strong>直接光照，反射方向间接光，折射方向间接光（如果有折射的话）</strong></p>
<p>整体过程是一个递归的过程，因此需要一定的递归终止条件，比如说允许的最大反射或折射次数为10。
光线在每次反射和折射之后都有能量损耗的，由系数决定，因此越往后的折射和反射光贡献的能量越小，这也是为什么在上文中提到根据光线能量权重求和。
e.g. 反射系数为0.7，那么第一次反射折损30%，第二次反射折损1-（70%x70%），依次类推。
如果反射或折射光线没有碰撞到物体，一般直接返回一个背景色。</p>
<pre><code>————————————————
版权声明：本文为CSDN博主「剑 来!」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_38065509/article/details/106299336
</code></pre>
<p>光线和AABB求交</p>
<p>光线全部进入时，计算进入时间，光线离开任意一对面，计算离开时间</p>
<p>对于每一对面，有进入时间t_min,离开时间t_max</p>
<p>进入时间(所有进入时间的最大值)：max(t_min)   负值是允许的，因为可能在盒子内部</p>
<p>离开时间(所有离开时间的最小值)：min(t_max)	负值在表明盒子在光线后边</p>
<p><img src="webgpu.assets/image-20231217210746094.png" alt="image-20231217210746094"></p>
<p><img src="webgpu.assets/image-20231217210354403.png" alt="image-20231217210354403"></p>
<p>KD-tree（划分空间）</p>
<p>物体可能存在于多个叶子结点</p>
<p>三角形不好判断求交</p>
<p>BVH（划分物体）</p>
<p><img src="webgpu.assets/image-20231218191224268.png" alt="image-20231218191224268"><img src="webgpu.assets/image-20231218191306397.png" alt="image-20231218191306397"></p>
<h3>辐射度量学</h3>
<p>球面积：</p>
<pre><code>4*pi*r^2
</code></pre>
<p><strong>Radiant Intensity</strong>：辐射强度 （单位立体角上的能量）</p>
<p>立体角：单位面积/半径平方  </p>
<p><img src="webgpu.assets/image-20231220215110690.png" alt="image-20231220215110690"></p>
<p>微分立体角：(两个角度-&gt;确定方向)<img src="webgpu.assets/image-20230616193049755.png" alt="image-20230616193049755"></p>
<p><strong>Irradiance</strong>：辐照度 （单位面积上的能量）</p>
<p>区别于intensity,下方的intensity不变(单位立体角没变)，但是irradiance在改变</p>
<p><img src="webgpu.assets/image-20230616193705785.png" alt="image-20230616193705785"></p>
<p><strong>Radiance</strong>：辐射度 </p>
<p><img src="webgpu.assets/image-20230616194457555.png" alt="image-20230616194457555"></p>
<h3>BRDF</h3>
<p>(由概率密度采样)<strong>均分能量</strong></p>
<p> <img src="webgpu.assets/image-20230616195023391.png" alt="image-20230616195023391"></p>
<p><img src="webgpu.assets/image-20230616195422498.png" alt="image-20230616195422498"></p>
<p><strong>递归</strong>   </p>
<h3>渲染方程(The Rendering Equation)</h3>
<p><img src="webgpu.assets/image-20230616195805016.png" alt="image-20230616195805016"></p>
<p><img src="webgpu.assets/image-20230616200759046.png" alt="image-20230616200759046"></p>
<p>在玻璃球中，光线弹射几次后出来 -&gt; 发亮，如下图玻璃灯</p>
<p><img src="webgpu.assets/image-20230616201429201.png" alt="image-20230616201429201"></p>
<h3>Monte Carlo Integration(蒙德卡洛路径积分)</h3>
<p>积分域采样</p>
<p><img src="webgpu.assets/image-20230616203645174.png" alt="image-20230616203645174"></p>
<p><img src="webgpu.assets/image-20230616203624642.png" alt="image-20230616203624642"></p>
<h3>Path Tracing    (N=1)</h3>
<p><img src="webgpu.assets/image-20230616210227181.png" alt="image-20230616210227181"></p>
<p><img src="webgpu.assets/image-20230616210039575.png" alt="image-20230616210039575"></p>
<p><strong>存在光线多次弹射后的光线数量爆炸的问题</strong></p>
<p>令N=1,即为<strong>路径追踪</strong></p>
<p><strong>俄罗斯轮盘赌决定光线是否弹射</strong></p>
<p><img src="webgpu.assets/image-20230616210832955.png" alt="image-20230616210832955"></p>
<p>但是，小光源可能采样不到</p>
<p><img src="webgpu.assets/image-20230616211046983.png" alt="image-20230616211046983"></p>
<p><strong>对光源采样：</strong></p>
<p><img src="webgpu.assets/image-20230616211314780.png" alt="image-20230616211314780"></p>
<p>变量替换：（对谁采样，对谁积分）</p>
<p><img src="webgpu.assets/image-20230616211505896.png" alt="image-20230616211505896"></p>
<p>最终版本：</p>
<p><img src="webgpu.assets/image-20230616211846793.png" alt="image-20230616211846793"></p>
<p>这色点打一根光线到光源，判定光源是否被遮挡：</p>
<p><img src="webgpu.assets/image-20230616212110632.png" alt="image-20230616212110632"></p>
<h1>GitHub</h1>
<h2>webgpu-seed</h2>
<p><img src="webgpu.assets/image-20230628201224469.png" alt="image-20230628201224469"></p>
<h2>burrow(去giuhub主页看<a href="https://toji.github.io/burrow/demo/">https://toji.github.io/burrow/demo/</a>)</h2>
<p><img src="webgpu.assets/image-20230628201728831.png" alt="image-20230628201728831"></p>
<h2>ecard</h2>
<p><strong>wrong file</strong></p>
<h2>evanw</h2>
<p><img src="webgpu.assets/image-20230628201908081.png" alt="image-20230628201908081"></p>
<h2>LearningWebGPU</h2>
<p><strong>Wrong</strong></p>
<h2>orillusion-web</h2>
<h2>orillusion-webgpu-samples-main</h2>
<h2>tumblin-down-main(rust Project)</h2>
<h2>webdnn</h2>
<p><strong>Wrong</strong></p>
<h2>WebGL-Fluid-Simulation</h2>
<p><img src="webgpu.assets/image-20230628202314129.png" alt="image-20230628202314129"></p>
<h2>webgpu-gltf</h2>
<p><img src="webgpu.assets/image-20230628202435515.png" alt="image-20230628202435515"></p>
<h2>webgpu-gltf-case-study</h2>
<p><img src="webgpu.assets/image-20230628202508939.png" alt="image-20230628202508939"></p>
<h2>WebGPU-Path-Tracer</h2>
<p><strong>wrong</strong></p>
<h2>webgpu-renderer</h2>
<p><strong>wrong</strong></p>
<h2>webgpu-renderer-gh-pages(MIKU)</h2>
<p><img src="webgpu.assets/image-20230628202747597.png" alt="image-20230628202747597"></p>
<h2>webgpu-samples</h2>
<p><img src="webgpu.assets/image-20230628202853716.png" alt="image-20230628202853716"></p>
<h2>webgpu-samples-main</h2>
<p><img src="webgpu.assets/image-20230628202944905.png" alt="image-20230628202944905"></p>
<h2>ricochet</h2>
<p><img src="webgpu.assets/image-20230705115204041.png" alt="image-20230705115204041"></p>
<h2><a href="https://github.com/AddisonPrairie/WebGPU-Path-Tracing">WebGPU-Path-Tracing</a>(WASM未配置环境)</h2>
<h2>Wonder-PBR-Offline-Render-Basic-Lecture-master</h2>
<pre><code>node .\lessons\coner_box\code\index.js
</code></pre>
<h2><strong><a href="https://github.com/amaiorano/raytracing-in-one-weekend-webgpu">raytracing-in-one-weekend-webgpu</a></strong></h2>
<p><img src="webgpu.assets/image-20230706152817220.png" alt="image-20230706152817220"></p>
<h2>MarsTracer</h2>
<p><img src="webgpu.assets/image-20230708174558055.png" alt="image-20230708174558055"></p>
<h2>RasterizeRayTrace</h2>
<p><img src="webgpu.assets/image-20230708174948298.png" alt="image-20230708174948298"></p>
<h2>webgpu-particles-master</h2>
<p><img src="webgpu.assets/image-20230902145222877.png" alt="image-20230902145222877"></p>
<h2>particles-webgpu-main</h2>
<p><img src="webgpu.assets/image-20230902150426474.png" alt="image-20230902150426474"></p>
<h2><strong><a href="https://github.com/matteokeole/webgpu_raytracer">webgpu_raytracer</a></strong></h2>
<p><img src="webgpu.assets/image-20230907140926411.png" alt="image-20230907140926411"></p>
<h2><strong><a href="https://github.com/tjjfvi/webgpu-test">webgpu-test</a></strong></h2>
<p><img src="webgpu.assets/image-20230907141126082.png" alt="image-20230907141126082"></p>
<p><strong><a href="https://github.com/antimatter15/splat">splat</a></strong></p>
<p><img src="webgpu.assets/image-20230912141200682.png" alt="image-20230912141200682"></p>
<h2><strong><a href="https://github.com/behunin/webgpu-bouy">webgpu-bouy</a></strong></h2>
<pre><code>https://github.com/behunin/webgpu-bouy.git
</code></pre>
<p><img src="webgpu.assets/image-20230926101739060.png" alt="image-20230926101739060"></p>
<h2><strong><a href="https://github.com/Popov72/OceanDemo">OceanDemo</a></strong></h2>
<p><img src="webgpu.assets/image-20230926102229624.png" alt="image-20230926102229624"></p>
<h2><a href="https://thimbleberry.dev/">Thimbleberry Image Transform Demo</a></h2>
<p><img src="webgpu.assets/image-20230927193429369.png" alt="image-20230927193429369"></p>
<h2><a href="https://thimbleberry.dev/">Thimbleberry Image Transform Demo</a></h2>
<p><img src="webgpu.assets/image-20230927193510769.png" alt="image-20230927193510769"></p>
<h2><a href="https://s-macke.github.io/WebGPU-Lab/">WebGPU-Lab</a></h2>
<p><img src="webgpu.assets/image-20230927193703209.png" alt="image-20230927193703209"></p>
<h2><a href="https://github.com/ghadeeras/ghadeeras.github.io">ghadeeras.github.io</a></h2>
<p><img src="webgpu.assets/image-20240107213730850.png" alt="image-20240107213730850"></p>
<h2><a href="https://github.com/Spiri0/Threejs-WebGPU-IFFT-Ocean-V1">Threejs-WebGPU-IFFT-Ocean-V1</a>](<a href="https://github.com/Spiri0/Threejs-WebGPU-IFFT-Ocean-V1">https://github.com/Spiri0/Threejs-WebGPU-IFFT-Ocean-V1</a>)</h2>
<p><img src="webgpu.assets/image-20240107213644916.png" alt="image-20240107213644916"></p>
<h2><strong><a href="https://github.com/antimatter15/splat">splat</a></strong></h2>
<p><img src="webgpu.assets/image-20240107223622670.png" alt="image-20240107223622670"></p>
<h1>常用API&amp;<a href="https://www.orillusion.com/zh/webgpu.html#pipelines">Webgpu from Official documents</a></h1>
<p><strong><a href="https://www.orillusion.com/zh/wgsl.html#enable-directive-section">WGSL from Official documents</a></strong></p>
<h2>device.createShaderModule</h2>
<p>用于创建着色器模块。着色器模块是一段编写的、运行在GPU上的代码，用于进行图形渲染或通用计算任务。</p>
<p>以下是关于<code>device.createShaderModule</code>方法的详细说明：</p>
<p><strong>语法：</strong></p>
<pre><code class="language-typescript">arduinoCopy code
const shaderModule = device.createShaderModule(descriptor);
</code></pre>
<p><strong>参数：</strong></p>
<ul>
<li><pre><code>descriptor
</code></pre>
<p>：着色器模块描述符，用于指定着色器代码和其他相关配置。它是一个包含以下属性的对象：</p>
<ul>
<li><code>code</code>（必需）：一个字符串，包含了着色器代码。通常使用GLSL（OpenGL Shading Language）或SPIR-V（Standard Portable Intermediate Representation for Vulkan）编写着色器代码。</li>
<li><code>sourceMap</code>（可选）：一个包含源映射信息的字符串，用于调试目的。</li>
<li><code>isEntry</code>（可选）：一个布尔值，指示着色器代码是否是入口点。如果为<code>true</code>，则表示该着色器模块可以作为渲染管线的入口。</li>
</ul>
</li>
</ul>
<p><strong>返回值：</strong></p>
<ul>
<li><code>shaderModule</code>：表示着色器模块的WebGPUShaderModule对象。可以将该对象与其他WebGPU API一起使用，如创建渲染管线、设置着色器程序等。</li>
</ul>
<h2>createBuffer</h2>
<p>在WebGPU中，<code>createBuffer</code> 方法用于创建一个GPU缓冲区对象，用于在GPU上存储数据。<code>mappedAtCreation</code> 是创建缓冲区时的一个属性，它决定了是否在创建缓冲区时就将其映射到CPU内存，以便进行数据的读写操作。</p>
<p>具体来说，<code>mappedAtCreation</code> 属性有两个可能的取值：</p>
<ol>
<li><code>true</code>：创建缓冲区的同时会将其映射到CPU内存，允许你在创建时就可以直接读写缓冲区数据。这可以用于方便地初始化缓冲区的数据，或者在数据变化时更新缓冲区。</li>
<li><code>false</code>（默认值）：创建缓冲区后，它不会立即映射到CPU内存。你需要使用另外的方法（例如 <code>getMappedRange</code>）来手动映射缓冲区，然后再进行数据的读写操作。这种方式可能会更高效，因为不会在创建时就产生映射和同步操作。</li>
</ol>
<p>使用 <code>mappedAtCreation</code> 可以根据你的需求来选择在创建缓冲区时是否将其映射到CPU内存。如果你需要在创建缓冲区后立即对数据进行修改，那么可以选择将其映射到CPU内存，方便直接读写数据。但请注意，映射缓冲区可能会带来一些性能开销，因此要权衡是否使用映射以及何时使用映射。</p>
<p>以下是一个示例，展示了如何在创建缓冲区时使用 <code>mappedAtCreation</code> 属性：</p>
<pre><code class="language-typescript">const device = /* 获取GPU设备的代码 */;

// 创建一个大小为 1024 字节的缓冲区，并映射到CPU内存
const bufferMapped = device.createBuffer({
    size: 1024,
    usage: GPUBufferUsage.STORAGE,
    mappedAtCreation: true
});

// 获取映射范围
const mappedRange = bufferMapped.getMappedRange();

// 在映射的范围内进行数据写入
const data = new Uint8Array(mappedRange);
data.fill(42);

// 取消映射
bufferMapped.unmap();

// 在之后的代码中，可以继续使用 bufferMapped
</code></pre>
<p>根据具体需求来决定是否在创建缓冲区时将其映射到CPU内存，以及何时取消映射。</p>
<p><img src="webgpu.assets/image-20230902160913150.png" alt="image-20230902160913150"></p>
<h2>device.createBindGroupLayout</h2>
<pre><code class="language-typescript">const uniformBindGroupLayout = device.createBindGroupLayout({
    entries: [
      {
        binding: 0,
        visibility: GPUShaderStage.VERTEX,
        buffer: {
          type: &#39;uniform&#39;,
        },
      },
      {
        binding: 1,
        visibility: GPUShaderStage.FRAGMENT,
        sampler: {},
      },
      {
        binding: 2,
        visibility: GPUShaderStage.FRAGMENT,
        texture: {},
      },
      {
        binding: 3,
        visibility: GPUShaderStage.FRAGMENT,
        texture: {},
      },
    ],
  });
</code></pre>
<h2>device.createPipelineLayout</h2>
<pre><code class="language-typescript">  const pipeLineLayout = device.createPipelineLayout({
    bindGroupLayouts: [uniformBindGroupLayout], // Add other layouts if needed
  });
</code></pre>
<h2>device.createBindGroup</h2>
<pre><code class="language-typescript">  const uniformBindGroup = device.createBindGroup({
    // layout: pipeline.getBindGroupLayout(0),
    layout: uniformBindGroupLayout,
    entries: [
      {
        binding: 0,
        resource: {
          buffer: uniformBuffer,
        },
      },
      {
        binding: 1,
        resource: sampler,
      },
      {
        binding: 2,
        resource: cubeTexture.createView(),
      },
      {
        binding: 3,
        resource: cubeTextureImg.createView(),
      },
    ],
  });
</code></pre>
<h2>GPURenderPipelineDescriptor</h2>
<pre><code>GPURenderPipelineDescriptor 具有以下成员：

vertex, of type GPUVertexState
描述了 管线 的顶点着色器入口点及其输入缓冲区布局。

primitive, of type GPUPrimitiveState, defaulting to {}
描述与 管线 相关的原始属性。

depthStencil, of type GPUDepthStencilState
描述了可选的深度模板属性，包括测试、操作和偏差。

multisample, of type GPUMultisampleState, defaulting to {}
描述 管线 的多重采样属性。

fragment, of type GPUFragmentState
描述了 管线 的片段着色器入口点及其输出颜色。如果没有 提供，则启用 § 23.3.8 无颜色输出 模式。

createRenderPipeline(descriptor)
创建一个使用即时管道创建的GPURenderPipeline。
</code></pre>
<pre><code>&#39;point-list&#39;: 对于每个顶点，绘制一个点
&#39;line-list&#39;: 每 2 个点绘制一条线
&#39;line-strip&#39;: 绘制最新点与前一点的连接线
&#39;triangle-list&#39;: 每 3 个点绘制一个三角形 (默认)
&#39;triangle-strip&#39;: 对于每个新位置，从它和最后 2 个位置中画出一个三角形
</code></pre>
<h3>primitive</h3>
<h4><strong>frontFace</strong></h4>
<ol>
<li><p>确定多边形是否是 front-facing 的，这取决于多边形在 <a href="https://www.orillusion.com/zh/webgpu.html#framebuffer">framebuffer</a> 坐标中占用的 area 的符号：</p>
<p>area = 0.5 × ((v1.x × vn.y − vn.x × v1.y) + ∑ (vi+1.x × vi.y − vi.x × vi+1.y))</p>
<p>area 的符号根据 <code>primitive</code>.<code>frontFace</code> 进行解释：</p>
<ul>
<li><p><code>&quot;ccw&quot;</code></p>
<p>area &gt; 0 被认为是 <a href="https://www.orillusion.com/zh/webgpu.html#front-facing">front-facing</a>，否则是 <a href="https://www.orillusion.com/zh/webgpu.html#back-facing">back-facing</a></p>
</li>
<li><p><code>&quot;cw&quot;</code></p>
<p>area &lt; 0 被认为是 <a href="https://www.orillusion.com/zh/webgpu.html#front-facing">front-facing</a>，否则是 <a href="https://www.orillusion.com/zh/webgpu.html#back-facing">back-facing</a></p>
</li>
</ul>
</li>
</ol>
<h4>cullMode</h4>
<ol>
<li><p>根据 <code>primitive</code>.<code>cullMode</code> 进行剔除：</p>
<ul>
<li><p><code>&quot;none&quot;</code></p>
<p>所有多边形都通过此测试。</p>
</li>
<li><p><code>&quot;front&quot;</code></p>
<p><a href="https://www.orillusion.com/zh/webgpu.html#front-facing">front-facing</a> 多边形被丢弃，并且不在渲染管线的后续阶段中处理。</p>
</li>
<li><p><code>&quot;back&quot;</code></p>
<p><a href="https://www.orillusion.com/zh/webgpu.html#back-facing">back-facing</a> 多边形被丢弃。</p>
</li>
</ul>
</li>
</ol>
<h2>createRenderPipelineAsync</h2>
<h2>draw(vertexCount, instanceCount, firstVertex, firstInstance)</h2>
<h2>createBindGroup</h2>
<p><strong>GPUBindGroupDescriptor</strong></p>
<p><code>GPUBindGroupDescriptor</code> 字典有以下成员：</p>
<ul>
<li><p><code>layout</code>, of type <a href="https://www.orillusion.com/zh/webgpu.html#gpubindgrouplayout">GPUBindGroupLayout</a></p>
<p><code>GPUBindGroupLayout</code> 该绑定组的条目将符合。</p>
</li>
<li><p><code>entries</code>, of type sequence&lt;<a href="https://www.orillusion.com/zh/webgpu.html#dictdef-gpubindgroupentry">GPUBindGroupEntry</a>&gt;</p>
<p>一个条目列表，描述为 <code>layout</code> 描述的每个绑定向着色器公开的资源。</p>
</li>
</ul>
<pre><code>typedef (GPUSampler or GPUTextureView or GPUBufferBinding or GPUExternalTexture) GPUBindingResource;

dictionary GPUBindGroupEntry {
    required GPUIndex32 binding;
    required GPUBindingResource resource;
};
</code></pre>
<p><code>GPUBindGroupEntry</code> 描述了要绑定在 <code>GPUBindGroup</code> 中的单个资源，并且具有 以下成员：</p>
<ul>
<li><p><code>binding</code>, of type <a href="https://www.orillusion.com/zh/webgpu.html#typedefdef-gpuindex32">GPUIndex32</a></p>
<p><code>GPUBindGroup</code> 中资源绑定的唯一标识符，对应于 <code>GPUBindGroupLayoutEntry.binding</code> 和 <code>GPUShaderModule</code> 中的 <a href="https://gpuweb.github.io/gpuweb/wgsl/#attribute-binding">@binding</a> 属性。</p>
</li>
<li><p><code>resource</code>, of type <a href="https://www.orillusion.com/zh/webgpu.html#typedefdef-gpubindingresource">GPUBindingResource</a></p>
<p>要绑定的资源，可以是 <code>GPUSampler</code>、<code>GPUTextureView</code>、<code>GPUExternalTexture</code> 或 <code>GPUBufferBinding</code>。</p>
</li>
</ul>
<h3></h3>
<h1>note from Github &amp; videos</h1>
<p><strong>安装typescript类型提示</strong></p>
<pre><code class="language-typescript">pnpm i --save-dev @types/dat.gui
</code></pre>
<p>检查设备maxBindGroups最大资源数量</p>
<pre><code class="language-typescript">const maxBindGroups = device.limits.maxBindGroups;
console.log(`Max Bind Groups: ${maxBindGroups}`);
</code></pre>
<h2>Csnvas</h2>
<ol>
<li><p><strong><code>&lt;canvas&gt;</code> 元素默认是行内元素</strong>：</p>
<p>默认情况下，<code>&lt;canvas&gt;</code> 元素是一个行内元素（inline element），其大小受到内容的影响。即使你在样式中设置了它的宽度和高度为100%，它仍然会按照默认的行内元素特性来渲染。</p>
</li>
<li><p><strong><code>&lt;canvas&gt;</code> 元素的默认基线</strong>：</p>
<p>行内元素会以其基线（baseline）对齐，这会导致 <code>canvas</code> 元素底部可能会超出视窗底部，从而触发纵向滚动条。</p>
</li>
</ol>
<p>为了解决这个问题，你可以将 <code>&lt;canvas&gt;</code> 元素的 <code>display</code> 属性设置为 <code>block</code>，将其转变为块级元素，使其能够充满其父容器，不再受到基线的影响：</p>
<pre><code class="language-css">canvas {
    display: block;
    width: 100%;
    height: 100%;
}
</code></pre>
<p>这样修改后，<code>&lt;canvas&gt;</code> 将会占满整个视窗，且不会出现 x、y 轴滚动条。</p>
<p><img src="webgpu.assets/image-20231215120157221.png" alt="image-20231215120157221"></p>
<h2>相机</h2>
<h3>涉及的API</h3>
<h4>Document：pointerLockElement 属性</h4>
<p><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document"><code>Document</code></a> 接口的 <code>pointerLockElement</code> 只读属性提供了指针锁定时鼠标事件的目标元素。如果指针处于锁定等待中、指针没有被锁定或目标元素在另外一个文档中，返回 <code>null</code>。</p>
<h4>Document: pointerlockchange event</h4>
<p><code>pointerlockchange</code> 事件当指针解锁或者被锁定时触发</p>
<h4>Element.requestPointerLock()</h4>
<p><strong>实验性:</strong> <strong>这是一项<a href="https://developer.mozilla.org/zh-CN/docs/MDN/Writing_guidelines/Experimental_deprecated_obsolete#%E5%AE%9E%E9%AA%8C%E6%80%A7">实验性技术</a></strong>
在将其用于生产之前，请仔细检查<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Element/requestPointerLock#%E6%B5%8F%E8%A7%88%E5%99%A8%E5%85%BC%E5%AE%B9%E6%80%A7">浏览器兼容性表格</a>。</p>
<p><strong><code>Element.requestPointerLock()</code></strong> 方法允许您异步地请求将鼠标指针锁定在指定元素上。</p>
<p>若想追踪请求成功还是失败，则需要在 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document"><code>Document</code></a> 级别监听 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document/pointerlockchange_event"><code>pointerlockchange</code></a> 和 <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/pointerlockerror_event"><code>pointerlockerror</code> (en-US)</a> 事件。</p>
<h4>Document.exitPointerLock()</h4>
<p><strong>实验性:</strong> <strong>这是一项<a href="https://developer.mozilla.org/zh-CN/docs/MDN/Writing_guidelines/Experimental_deprecated_obsolete#%E5%AE%9E%E9%AA%8C%E6%80%A7">实验性技术</a></strong>
在将其用于生产之前，请仔细检查<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document/exitPointerLock#%E6%B5%8F%E8%A7%88%E5%99%A8%E5%85%BC%E5%AE%B9%E6%80%A7">浏览器兼容性表格</a>。</p>
<p><code>exitPointerLock</code> 方法可异步的解锁鼠标（通过<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Element/requestPointerLock"><code>Element.requestPointerLock</code></a>锁定的）。</p>
<p>追踪是否解锁成功，需要监听<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document/pointerlockchange_event"><code>pointerlockchange</code></a> 和<a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/pointerlockerror_event"><code>pointerlockerror</code> (en-US)</a> 事件。</p>
<h4>MouseEvent()</h4>
<p><strong><code>MouseEvent()</code></strong> 构造器创建一个 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/MouseEvent"><code>MouseEvent</code></a>。</p>
<h5><strong>MouseEvent.movementX</strong></h5>
<p><strong><code>MouseEvent.movementX</code></strong> 是只读属性，它提供了当前事件和上一个<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Element/mousemove_event"><code>mousemove</code></a>事件之间鼠标在水平方向上的移动值。换句话说，这个值是这样计算的 : <code>currentEvent.movementX = currentEvent.screenX - previousEvent.screenX</code>.</p>
<h5>MouseEvent.movementY</h5>
<p><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/MouseEvent"><code>MouseEvent</code></a> 接口的 <strong><code>MouseEvent.movementY</code></strong> 只读属性提供了当前事件和上一个 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Element/mousemove_event"><code>mousemove</code></a> 事件之间鼠标在竖直方向上的移动值。换句话说，这个值是这样计算的：<code>currentEvent.movementY = currentEvent.screenY - previousEvent.screenY</code>。</p>
<h2><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/MouseEvent/MouseEvent#%E8%AF%AD%E6%B3%95">语法</a></h2>
<pre><code> event = new MouseEvent(typeArg, mouseEventInit);
</code></pre>
<h3><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/MouseEvent/MouseEvent#%E5%BD%A2%E5%8F%82">形参</a></h3>
<ul>
<li><p><em>typeArg</em></p>
<p><a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/String"><code>DOMString</code></a> 格式的事件名称。</p>
</li>
<li><p><em>mouseEventInit</em> 可选</p>
<p><img src="webgpu.assets/image-20230706104219188.png" alt="image-20230706104219188"></p>
</li>
</ul>
<h2>数学</h2>
<h3>向量叉积（vec3.cross）</h3>
<p>在计算机图形学中，<code>normal</code>代表了四边形面的法向量。法向量是垂直于平面的向量，它的方向指向平面的外部。在三维渲染中，法向量是非常重要的属性，因为它决定了表面的朝向和光照计算的结果。</p>
<p>通过使用向量的叉乘操作 <code>vec3.cross(quad.right, quad.up)</code>，可以计算出两个向量的叉乘结果，该结果即为法向量。叉乘操作的结果是一个与给定两个向量都垂直的向量，它的长度表示了两个向量确定的平行四边形的面积。通过对该结果进行归一化操作 <code>vec3.normalize</code>，可以将法向量转化为单位向量，使其长度为1。</p>
<p>在三维渲染中，法向量用于表面的光照计算和阴影效果的生成。光照计算中，法向量与光线的方向向量进行点乘操作，可以得到光照的强度，从而确定表面的亮度和阴影。法向量还用于判断面的朝向，决定面的可见性和渲染顺序。</p>
<p>总结而言，<code>normal</code>代表四边形面的法向量，它在三维渲染中用于光照计算、阴影生成和面的朝向判断等方面，对于呈现真实的三维效果非常重要。</p>
<h3>点乘操作(vec3.dot)</h3>
<pre><code>`-vec3.dot(normal, quad.center)` 表示法向量 `normal` 与四边形的中心点坐标 `quad.center` 的点乘操作，并取负值。

1. 首先，计算法向量 normal 和四边形中心点向量 quad.center 的点乘操作。
2. vec3.dot(normal, quad.center) 表示将法向量 normal 和中心点向量 quad.center 的对应分量相乘，并将结果相加得到一个标量值。
3. 最后，取该标量值的负值，即 -vec3.dot(normal, quad.center)。

点乘操作是一种向量运算，它计算两个向量之间的乘积。点乘操作的结果是一个标量（即数量），其数值表示了两个向量之间的相似程度或关联程度。在三维空间中，点乘操作可以用来判断两个向量的方向是否相似或相反，从而用于计算光照、阴影和投影等效果。
</code></pre>
<p><strong>在这里，<code>-vec3.dot(normal, quad.center)</code> 的结果表示了法向量 <code>normal</code> 和四边形中心点 <code>quad.center</code> 的关联程度。由于取了负值，结果的数值可以用于表示四边形相对于法向量的偏移量。如果结果为正值，则表示中心点在法向量所指向的那一侧；如果结果为负值，则表示中心点在法向量相反的那一侧；如果结果为0，则表示中心点位于法向量所在的平面上。</strong></p>
<p><strong>在渲染中，这个值可以用于光照计算、阴影生成和投影效果等方面。例如，在光照计算中，这个值可以用来判断光源和表面的相对位置，从而确定表面的亮度和阴影效果。在投影效果中，这个值可以用于判断物体在投影平面上的投影位置。</strong></p>
<p><strong>总结而言，<code>-vec3.dot(normal, quad.center)</code> 表示法向量和四边形中心点的关联程度，可以用于确定物体在光照、阴影和投影等方面的位置和效果。</strong></p>
<h4>中心点向量</h4>
<pre><code>中心点向量的方向是由原点（或参考点）指向四边形的中心点的方向。它表示了从原点（或参考点）到四边形中心点的方向和距离。
</code></pre>
<h3>mat4.lookat()</h3>
<p><strong>通常情况下，在计算机图形学中，视图矩阵的初始状态是将观察者从原点 <code>(0, 0, 0)</code> 沿着负 Z 轴方向观察场景。这意味着观察者位于原点，并且面向负 Z 轴方向。</strong></p>
<p>通过使用 <code>mat4.lookAt</code> 函数生成的视图矩阵，可以将世界空间中的坐标转换为相机空间中的坐标，从而实现以相机视角观察场景的效果。通过将视图矩阵与世界空间中的坐标相乘，可以将这些坐标从全局坐标系转换为相对于相机位置和方向的局部坐标系。这样，渲染器在进行场景渲染时就会根据相机的位置和方向进行正确的投影和可视化。</p>
<p>需要注意的是，使用 <code>mat4.lookAt</code> 函数生成的视图矩阵还需要与投影矩阵结合使用，以实现完整的透视投影和视景体裁剪效果。投影矩阵定义了视景体的形状和远近裁剪平面，而视图矩阵则定义了观察者的位置和方向。通过将这两个矩阵相乘，可以将场景中的顶点从世界空间转换为最终的屏幕空间坐标，从而进行正确的渲染和可视化。</p>
<h3>mat4.perspective</h3>
<p><code>mat4.perspective</code> 函数实现透视投影，</p>
<p><code>mat4.perspective</code> 函数接受一些参数，例如视野角度（fov）、宽高比（aspect ratio）、近平面距离和远平面距离等，用于定义透视投影矩阵。通过将透视投影矩阵与视图矩阵相乘，可以将场景中的顶点从世界空间转换为最终的屏幕空间坐标，从而实现透视投影效果。</p>
<p>与透视投影相对，正交投影（Orthographic Projection）是一种投影方式，它不考虑物体与观察者的距离，使得物体在不同距离上保持相同的大小。正交投影通常用于需要保持物体大小不变的场景，例如二维游戏、技术绘图等。</p>
<p>因此，<code>mat4.perspective</code> 函数定义了视景体的形状和远近裁剪平面，并实现了透视投影。</p>
<h3>mat4.ortho</h3>
<p><img src="webgpu.assets/image-20230712101932088.png" alt="image-20230712101932088"></p>
<h2>内置变量</h2>
<p>传递数据：顶点插槽  bindgroup</p>
<p>uniform buffer  64kb  shader中只读</p>
<p>storage buffer   很大   且shader中可读可写</p>
<p>@vertex_index(内置)第几个顶点</p>
<p>@instance_index(内置)第几个实例</p>
<pre><code>export default class HObject {
  public static CLASS_NAME: string = &#39;HObject&#39;;
  public isHObject: boolean = true;
  public name: string;
  
  /**
   * 获取对象的ID
   */
  get id(): number;

  /**
   * 获取对象的哈希值
   */
  get hash(): string;
}

export default class RenderTexture extends HObject {
  /**
   * 判断给定的值是否为RenderTexture类型
   * @param value 要检查的值
   * @returns 如果值为RenderTexture类型，则返回true；否则返回false
   */
  public static IS(value: any): value is RenderTexture {
    return !!value.isRenderTexture;
  }

  public static CLASS_NAME: string = &#39;RenderTexture&#39;;
  public isRenderTexture: boolean = true;
}
</code></pre>
<p>对于 <code>value is RenderTexture</code> 这个语法，它是 TypeScript 中的类型保护机制的一部分。这个语法表示在 <code>IS</code> 方法中，我们希望判断给定的 <code>value</code> 是否是 <code>RenderTexture</code> 类型的实例。如果条件成立，TypeScript 将会推断出在 <code>IS</code> 方法的返回值中，<code>value</code> 是 <code>RenderTexture</code> 类型。这可以帮助我们进行类型检查和类型推断。</p>
<p><code>!!value.isRenderTexture</code> 这个表达式中的 <code>!!</code> 是逻辑非操作符的两次连续使用。它的作用是将表达式的结果转换为布尔值。如果 <code>value.isRenderTexture</code> 的值为真，则 <code>!!value.isRenderTexture</code> 的结果为 <code>true</code>。否则，结果为 <code>false</code>。在这里，它用于判断对象 <code>value</code> 中的 <code>isRenderTexture</code> 属性是否存在且为真。</p>
<p>似乎是依次绘制，就像画画一样，但是最好对于一个顶点数据，最好一次画完，切换管线开销很大</p>
<p><img src="webgpu.assets/image-20230626184825680.png" alt="image-20230626184825680"></p>
<h2>多物体资源绑定</h2>
<p><img src="webgpu.assets/image-20230817203113025.png" alt="image-20230817203113025"></p>
<p><strong>通过setBindGroup可以将资源绑定到管线的vertexshader和fragmentshader中，和setvertexBuffer不同的是，这种setBindGroup相当于全局变量，一个group可以在一个管线的vertex shader 和fragment shader中共享数据，也可以绑定在不同的pipeline，在不同的pipeline中共享数据</strong></p>
<p><strong>webgpu坐标轴z是0~1</strong></p>
<p><img src="webgpu.assets/image-20230817213152710.png" alt="image-20230817213152710"></p>
<p><strong>webgpu中，物体的最终情况只和顶点数据和mvp矩阵有关，两个setBindGroup相当于绘制了两次</strong></p>
<p><strong>方案二三其实都要256字节对齐，方案四在shader里操作index移动指针，只需要移动16/64字节，速度会快一点，而且方案一二三，用js切换mvpbuffer，其实在js侧的时间消耗都比较大，最好应该在js和cpu中提前准备好数据，再一次性写入GPU</strong></p>
<h3>方案一：</h3>
<p>每次绘制都有相应的buffer,group</p>
<h3>方案二：</h3>
<p>mvp全部放在一个大的buffer   注意对其，多个group绑定在一个buffer,webgpu要求256字节强制对齐</p>
<p><img src="webgpu.assets/image-20230626185237915.png" alt="image-20230626185237915"></p>
<h3>方案三：不对齐，利用特性dynamicOffset</h3>
<p><img src="webgpu.assets/image-20230626185604087.png" alt="image-20230626185604087"></p>
<p>之后就可以传入group,但是要加上一个参数：数组offect,下图[0]表示第一个cube的offect为0，第二个cube的offect为256，也就是方案二是提前设置好offset,方案三是在renderpass里动态设置offset</p>
<p><img src="webgpu.assets/image-20230626185635795.png" alt="image-20230626185635795"></p>
<p><strong>疑问：</strong>
<img src="webgpu.assets/image-20230626190146450.png" alt="image-20230626190146450"></p>
<h3>方案四：（并行）</h3>
<p>这里是把36个线程当做一个组，同时开启NUM组起去运行，如果NUM=100,相当于整个管线要被运行3600次</p>
<p><img src="webgpu.assets/image-20230626190546194.png" alt="image-20230626190546194"></p>
<p>无法切换数据，</p>
<p>在shader中：</p>
<p>对应的第几个顶点:</p>
<p><img src="webgpu.assets/image-20230626191211619.png" alt="image-20230626191211619"></p>
<p>本质上是把buffer和group的切换放到了shader里，但是group可以靠shader来切换，但是顶点信息却没有办法去切换，也就是适合绘制同顶点数据的场景</p>
<p><img src="webgpu.assets/image-20230626191717751.png" alt="image-20230626191717751"></p>
<h3>总结</h3>
<p><img src="webgpu.assets/image-20230626192738197.png" alt="image-20230626192738197"></p>
<h2>texture</h2>
<h3>createTexture</h3>
<pre><code>在WebGPU中，`createTexture` 方法的 `size` 参数中的 `depthOrArrayLayers` 表示创建的纹理的深度或数组层数。它用于指定纹理的维度信息。

具体取值有两种情况：

1. 对于 2D 纹理，`depthOrArrayLayers` 表示数组层数，即纹理数组的长度。如果只需要创建单个纹理，该值为 1。

2. 对于 3D 纹理，`depthOrArrayLayers` 表示纹理的深度，即纹理在 Z 轴方向上的层数。

需要注意的是，纹理的深度或数组层数取决于纹理的类型和使用情况。在创建纹理时，需要根据实际需求选择合适的值来定义纹理的尺寸和维度。
</code></pre>
<h3>纹理的维度</h3>
<p>在计算机图形学中，纹理的维度指的是纹理的空间维度，用于描述纹理数据的排列方式和访问方式。常见的纹理维度包括：</p>
<ol>
<li><p>1D 纹理（一维纹理）：纹理数据在一个维度上排列，可以通过一个浮点数或整数索引进行访问，例如用于纹理映射和采样的灰度图像。</p>
</li>
<li><p>2D 纹理（二维纹理）：纹理数据在两个维度上排列，通常用于表示彩色图像或二维图案。可以通过二维坐标进行访问，例如用于纹理映射的贴图。</p>
</li>
<li><p>3D 纹理（三维纹理）：纹理数据在三个维度上排列，用于表示立体图像、体积数据或多个层次的纹理数据。可以通过三维坐标进行访问，例如用于体积渲染的纹理。</p>
</li>
<li><p>立方体纹理（立方体贴图）：由六个 2D 纹理组成，每个面都是一个 2D 纹理。用于表示环境映射等需要多个方向的纹理信息的情况。</p>
</li>
</ol>
<p>纹理的维度决定了纹理数据的排列方式和访问方式。在使用 WebGPU 的 <code>createTexture</code> 方法创建纹理时，需要根据所需的纹理维度来选择合适的纹理类型和相应的参数。</p>
<h3>textureSample</h3>
<p><strong>导入图片</strong></p>
<pre><code class="language-typescript">  let cubeTextureImg: GPUTexture;
  {
    const response = await fetch(
      new URL(&#39;../../../assets/img/Di-3d.png&#39;, import.meta.url).toString()
    );
    const imageBitmap = await createImageBitmap(await response.blob());

    cubeTextureImg = device.createTexture({
      size: [imageBitmap.width, imageBitmap.height, 1],
      format: &#39;rgba8unorm&#39;,
      usage:
        GPUTextureUsage.TEXTURE_BINDING |
        GPUTextureUsage.COPY_DST |
        GPUTextureUsage.RENDER_ATTACHMENT,
    });
    device.queue.copyExternalImageToTexture(
      { source: imageBitmap },
      { texture: cubeTextureImg },
      [imageBitmap.width, imageBitmap.height]
    );
  }
</code></pre>
<p><strong>采样器</strong></p>
<pre><code class="language-typescript">  const sampler = device.createSampler({
    magFilter: &#39;linear&#39;,
    minFilter: &#39;linear&#39;,
  });
</code></pre>
<p><strong>绑定组</strong></p>
<pre><code class="language-typescript">const uniformBindGroup = device.createBindGroup({
    // layout: pipeline.getBindGroupLayout(0),
    layout: uniformBindGroupLayout,
    entries: [
      {
        binding: 0,
        resource: {
          buffer: uniformBuffer,
        },
      },
      {
        binding: 1,
        resource: sampler,
      },
      {
        binding: 2,
        resource: cubeTexture.createView(),
      },
      {
        binding: 3,
        resource: cubeTextureImg.createView(),
      },
    ],
  });
</code></pre>
<h4>fractalCube</h4>
<p>将上一帧的结果通过：</p>
<pre><code class="language-typescript">  commandEncoder.copyTextureToTexture(
      {
        texture: swapChainTexture,
      },
      {
        texture: cubeTexture,
      },
      [canvas.width, canvas.height]
    );
</code></pre>
<p>复制，并且将cubeTexture传入片段插槽：</p>
<pre><code>@binding(2) @group(0) var myTexture: texture_2d&lt;f32&gt;;
</code></pre>
<h2>并行计算</h2>
<p><img src="webgpu.assets/image-20230628102521824.png" alt="image-20230628102521824"></p>
<p><img src="webgpu.assets/image-20230628102423400.png" alt="image-20230628102423400"></p>
<p><img src="webgpu.assets/image-20230628102552241.png" alt="image-20230628102552241"></p>
<h2>缓冲区读写</h2>
<pre><code class="language-typescript">await readBuffer.mapAsync(GPUMapMode.READ)

const copyArrayBuffer = readBuffer.getMappedRange()
const result = new Float32Array(copyArrayBuffer)

readBuffer.unmap()
</code></pre>
<h2>相机控件</h2>
<pre><code class="language-typescript">import * as glm from &quot;gl-matrix&quot;;
glm.glMatrix.setMatrixArrayType(Array);

export default class Camera {
    canvas: HTMLCanvasElement;
    verticalFOV: number = 45;
    nearClip: number = 0.1;
    farClip: number = 100;
    translationSpeed: number = 5;
    rotationSpeed: number = 3;

    projection: glm.mat4 = glm.mat4.create();
    view: glm.mat4 = glm.mat4.create();
    inverseProjection: glm.mat4 = glm.mat4.create();
    inverseView: glm.mat4 = glm.mat4.create();

    position: glm.vec3 = glm.vec3.create();
    forwardDirection: glm.vec3 = glm.vec3.create();
    upDirection: glm.vec3 = glm.vec3.create();

    move = false;
    translateX = 0;
    translateY = 0;
    translateZ = 0;

    rotate = false;
    mouseMovement = [0, 0];

    constructor(
        canvas: HTMLCanvasElement,
        verticalFOV: number,
        nearClip: number,
        farClip: number,
        translationSpeed: number
    ) {
        this.canvas = canvas;
        this.verticalFOV = verticalFOV;
        this.nearClip = nearClip;
        this.farClip = farClip;
        this.translationSpeed = translationSpeed;
        this.rotationSpeed = 3;

        this.forwardDirection = glm.vec3.fromValues(0, 0, -1);
        this.upDirection = glm.vec3.fromValues(0, 1, 0);
        this.position = glm.vec3.fromValues(0, 0, 20);

        this.recalculateProjection();
        this.recalculateView();

        document.addEventListener(&quot;mousedown&quot;, async (e) =&gt; {
            // right click
            if (e.button === 2) {
                if (!document.pointerLockElement) {
                    try {
                        // @ts-ignore, function DOES take param according to mdn
                        await canvas.requestPointerLock({ unadjustedMovement: true });
                    } catch {
                        alert(
                            &quot;Please left click the view before trying to move with right click.&quot;
                        );
                    }
                }
            }
        });

        document.addEventListener(&quot;mouseup&quot;, () =&gt; {
            document.exitPointerLock();
        });

        document.addEventListener(&quot;pointerlockchange&quot;, () =&gt; {
            if (document.pointerLockElement == canvas) {
                console.log(&#39;pointerlockchange&#39;);                
                this.move = true;
                document.addEventListener(&quot;mousemove&quot;, getMouseMovement);
                document.addEventListener(&quot;keydown&quot;, getKeyDown);
                document.addEventListener(&quot;keyup&quot;, getKeyUp);
            } else {
                this.move = false;
                document.removeEventListener(&quot;mousemove&quot;, getMouseMovement);
                document.removeEventListener(&quot;keydown&quot;, getKeyDown);
                // we don&#39;t remove up so we can see a keyup after pointer unlock
            }
        });

        const getKeyDown = (e: KeyboardEvent) =&gt; {
            // console.log(e.code); 
            if (e.code === &quot;KeyW&quot;) this.translateZ = 1;           
            else if (e.code === &quot;KeyS&quot;) this.translateZ = -1;

            if (e.code === &quot;KeyD&quot;) this.translateX = 1;
            else if (e.code === &quot;KeyA&quot;) this.translateX = -1;

            if (e.code === &quot;KeyE&quot;) this.translateY = 1;
            else if (e.code === &quot;KeyQ&quot;) this.translateY = -1;
        };

        const getKeyUp = (e: KeyboardEvent) =&gt; {
            if (e.code === &quot;KeyW&quot;) this.translateZ = 0;
            else if (e.code === &quot;KeyS&quot;) this.translateZ = 0;

            if (e.code === &quot;KeyD&quot;) this.translateX = 0;
            else if (e.code === &quot;KeyA&quot;) this.translateX = 0;

            if (e.code === &quot;KeyE&quot;) this.translateY = 0;
            else if (e.code === &quot;KeyQ&quot;) this.translateY = 0;
        };

        const getMouseMovement = (e: MouseEvent) =&gt; {
            this.mouseMovement[0] += e.movementX;
            this.mouseMovement[1] += e.movementY;
            // console.log(this.mouseMovement);            
        };
    }

    updatePos = () =&gt; {
        if (this.move) {
            // translate
            let rightDirection = glm.vec3.create();
            // console.log(rightDirection);            
            glm.vec3.cross(rightDirection, this.forwardDirection, this.upDirection);
            if (this.translateX !== 0)
                glm.vec3.scaleAndAdd(
                    this.position,
                    this.position,
                    rightDirection,
                    this.translationSpeed * this.translateX // * ts
                );
            if (this.translateY !== 0)
                glm.vec3.scaleAndAdd(
                    this.position,
                    this.position,
                    this.upDirection,
                    this.translationSpeed * this.translateY // * ts
                );
            if (this.translateZ !== 0)
                glm.vec3.scaleAndAdd(
                    this.position,
                    this.position,
                    this.forwardDirection,
                    this.translationSpeed * this.translateZ // * ts
                );

            // rotate
            glm.vec3.cross(rightDirection, this.forwardDirection, this.upDirection);
            
            // 这两行是我注释的
            rightDirection = glm.vec3.create();   
            glm.vec3.cross(rightDirection, this.forwardDirection, this.upDirection);
            
            let delta = glm.vec2.fromValues(
                this.mouseMovement[0],
                this.mouseMovement[1]
            );
            glm.vec2.scale(delta, delta, 0.001);

            let pitchDelta = delta[1] * this.rotationSpeed;
            let yawDelta = delta[0] * this.rotationSpeed;

            let pitchQuat = glm.quat.create();
            glm.quat.setAxisAngle(pitchQuat, rightDirection, -pitchDelta);      
            let yawQuat = glm.quat.create();
            glm.quat.setAxisAngle(yawQuat, this.upDirection, -yawDelta);

            let q = glm.quat.create();
            glm.quat.mul(q, pitchQuat, yawQuat);
            glm.quat.normalize(q, q);
            glm.vec3.transformQuat(this.forwardDirection, this.forwardDirection, q);
            // console.log(this.forwardDirection);            
            this.recalculateProjection();
            this.recalculateView();

            this.mouseMovement = [0, 0];
        }
    };

    recalculateProjection = () =&gt; {
        glm.mat4.perspectiveZO(
            this.projection,
            this.verticalFOV,
            this.canvas.width / this.canvas.height,
            this.nearClip,
            this.farClip
        );
        glm.mat4.invert(this.inverseProjection, this.projection);
    };

    recalculateView = () =&gt; {
        let positionPlusForward = glm.vec3.create();
        glm.vec3.add(positionPlusForward, this.position, this.forwardDirection);
        glm.mat4.lookAt(
            this.view,
            this.position,
            positionPlusForward,
            glm.vec3.fromValues(0, 1, 0)
        );
        glm.mat4.invert(this.inverseView, this.view);
    };
}
</code></pre>
<pre><code>import Camera from &#39;./util/camera&#39;
</code></pre>
<p><strong>view project矩阵</strong></p>
<pre><code class="language-typescript">const camera = new Camera(
    canvas,
    Math.PI / 6,
    0.1,
    100,
    0.5);

function frame(){

    // control
    let ModelMatrix = getModelViewMatrix(position, rotation, scale);
    let MVPMatrix = mat4.create()
    mat4.multiply(MVPMatrix, camera.projection, camera.view)
    mat4.multiply(MVPMatrix, MVPMatrix, ModelMatrix)
    // 将mat4矩阵转换为Float32Array
    const MVPMatrixArray = new Float32Array(MVPMatrix);

    device.queue.writeBuffer(
        pipelineObj.mvpBuffer,
        0,
        MVPMatrixArray.buffer
    )
    // 更新相机位置
    camera.updatePos();

    draw(device, context, pipelineObj)
    requestAnimationFrame(frame)
}
</code></pre>
<pre><code>    const camera = new Camera(
        canvas,
        Math.PI / 6,
        0.1,
        100,
        0.5);

    let MVPMatrix = mat4.create()

       mat4.multiply(MVPMatrix, camera.projection, camera.view)
        updateCamera(MVPMatrix)
        camera.updatePos();


        const MVPMatrixArray = new Float32Array(MVPMatrix);
        const aspect = size.width / size.height
        const projectionMatrix = getProjectionMatrix(aspect, 60 / 180 * Math.PI, 0.1, 1000, { x: 0, y: 10, z: 20 })
        device.queue.writeBuffer(pipelineObj.cameraProjectionBuffer, 0, projectionMatrix)
        // device.queue.writeBuffer(pipelineObj.cameraProjectionBuffer, 0, MVPMatrixArray.buffer)
</code></pre>
<h2>Resize</h2>
<p>注意配置精度，以及更新相机的参数(宽高比)</p>
<p><strong>方案一</strong></p>
<pre><code class="language-typescript">    const currentWidth = canvas.clientWidth * devicePixelRatio;
    const currentHeight = canvas.clientHeight * devicePixelRatio;
    The canvas size is animating via CSS.
    When the size changes, we need to reallocate the render target.
    We also need to set the physical size of the canvas to match the computed CSS size.
    if (
      (currentWidth !== canvas.width || currentHeight !== canvas.height) &amp;&amp;
      currentWidth &amp;&amp;
      currentHeight
    ) {
      if (depthObj.depthTexture !== undefined) { 
        // Destroy the previous render target
        depthObj.depthTexture.destroy();
      }

      // Setting the canvas width and height will automatically resize the textures returned
      // when calling getCurrentTexture() on the context.
      canvas.width = currentWidth;
      canvas.height = currentHeight;

      // Resize the multisampled render target to match the new canvas size.
      depthObj.depthTexture = device.createTexture({
        size: [canvas.width, canvas.height],
        format: &quot;depth32float&quot;,
        // format,
        usage: GPUTextureUsage.RENDER_ATTACHMENT,
      });

      depthObj.depthView = depthObj.depthTexture.createView();
    }
</code></pre>
<p><strong>方案二</strong></p>
<pre><code class="language-typescript">  // re-configure context on resize
  window.addEventListener(&#39;resize&#39;, ()=&gt;{
    console.log(&#39;resize&#39;);    
      size.width = canvas.width = canvas.clientWidth * devicePixelRatio
      size.height = canvas.height = canvas.clientHeight * devicePixelRatio
      
      // don&#39;t need to recall context.configure() after v104
      // re-create depth texture
      depthObj.depthTexture.destroy();

      depthObj.depthTexture = device.createTexture({
        size, format: &#39;depth32float&#39;,
        usage: GPUTextureUsage.RENDER_ATTACHMENT,
      });
      depthObj.depthView = depthObj.depthTexture.createView();
  })
</code></pre>
<h2>性能监测</h2>
<pre><code>作者：德育处主任
链接：https://juejin.cn/post/7139695087893610504
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
</code></pre>
<p><strong>引入 Stats</strong></p>
<pre><code class="language-typescript">在使用 npm install three  下载的依赖包中已经包含了 Stats.js 了
可以这样引入到项目中
js复制代码import Stats from &#39;three/examples/jsm/libs/stats.module.js&#39;


使用 Stats
要使用 Stats 需要做以下几步操作：

实例化 Stats
设置初始面板 stats.setMode(0) 。传入面板id（0: fps, 1: ms, 2: mb）
设置监视器的位置
将监视器添加到页面中
刷新帧数 stats.update()
</code></pre>
<pre><code class="language-typescript">代码如下所示
js复制代码// 创建性能监视器
let stats = new Stats()

// 设置监视器面板，传入面板id（0: fps, 1: ms, 2: mb）
stats.setMode(0)

// 设置监视器位置
stats.domElement.style.position = &#39;absolute&#39;
stats.domElement.style.left = &#39;0px&#39;
stats.domElement.style.top = &#39;0px&#39;

// 将监视器添加到页面中
document.body.appendChild(stats.domElement)

function render() {

  // 更新帧数
  stats.update()

  // 渲染界面
  renderer.render(scene, camera)
  requestAnimationFrame(render)
}

render()

当点击该面板时还可以切换监听的类型
</code></pre>
<pre><code class="language-typescript">设置初始展示类型的另一个方法
除了使用 setMode 设置初始展示的类型外，还可以使用 showPanel 设置。
比如设置初始展示的类型是 ms ，就传个 1 进去即可。

stats.showPanel(1)
</code></pre>
<pre><code class="language-typescript">更新监听数据的另一个方法
除了使用 update 更新监听到的数据变化之外，还可以使用 begin 和 end 组合。
js复制代码stats.begin()
// 中间夹着要监听的动画或其他代码
stats.end()

begin 和 end 中间可以夹着要统计的代码，它们会帮你计算出这段代码执行的平均帧数。
</code></pre>
<p><img src="webgpu.assets/image-20230902201056588.png" alt="image-20230902201056588"></p>
<p>read-only-storage得在uniform前面？？？</p>
<h2>粒子</h2>
<p><img src="webgpu.assets/image-20230904103614221.png" alt="image-20230904103614221"></p>
<h1>webgpu-fundamentals</h1>
<pre><code>https://webgpufundamentals.org/webgpu/lessons/zh_cn/webgpu-fundamentals.html#toc
</code></pre>
<h1>WGSL</h1>
<h2>基础语法</h2>
<p><img src="webgpu.assets/image-20230817192857530.png" alt="image-20230817192857530"></p>
<h2>MSAA</h2>
<p><img src="webgpu.assets/image-20230817194035483.png" alt="image-20230817194035483"></p>
<p><img src="webgpu.assets/image-20230817194051006.png" alt="image-20230817194051006"></p>
<p><img src="webgpu.assets/image-20230817194121263.png" alt="image-20230817194121263"></p>
<pre><code>    multisample: {
      count: 4,
    },
    
    
     const texture = device.createTexture({
    size: [canvas.width, canvas.height],
    sampleCount,
    format: presentationFormat,
    usage: GPUTextureUsage.RENDER_ATTACHMENT,
  });
  const view = texture.createView();
  
  colorAttachments: [
        {
          view,
          resolveTarget: context.getCurrentTexture().createView(),
          clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },
          loadOp: &#39;clear&#39;,
          storeOp: &#39;discard&#39;,
        },
      ],
</code></pre>
<p><strong>17.1.1.2. 深度/模板附件</strong><code>depthStencilAttachment</code></p>
<h2>内置变量</h2>
<h3>@builtin(position)</h3>
<p>@builtin(position) 在顶点着色器和片段着色器中的意义是不同的。</p>
<p>在顶点着色器中，<code>@builtin(position)</code> 是 GPU 绘制三角形/线/点所需的输出。</p>
<p>在片段着色器中，<code>@builtin(position)</code> 是一个输入。它是片段着色器当前被要求计算颜色的<strong>像素坐标</strong>。(范围应该是屏幕上的像素坐标)</p>
<p>像素坐标由像素边缘指定。而提供给片段着色器的值则是像素的中心坐标。</p>
<p>如果我们要绘制的纹理大小为 3x2 像素，下图中的黑点表示的则是片段着色器中的坐标。</p>
<p><img src="webgpu.assets/image-20230926111631034.png" alt="image-20230926111631034"></p>
<h2>内置函数</h2>
<p>From GPT:
根据目录中的函数名称，我可以大致猜测它们的作用：</p>
<p>16.3 逻辑内置函数：</p>
<ul>
<li>16.3.1 <code>all</code> - 判断所有元素是否为真</li>
<li>16.3.2 <code>any</code> - 判断任意一个元素是否为真</li>
<li>16.3.3 <code>select</code> - 从两个值中选择一个，取决于条件是否为真</li>
</ul>
<p>16.4 数组内置函数：</p>
<ul>
<li>16.4.1 <code>arrayLength</code> - 获取数组的长度</li>
</ul>
<p>16.5 数值内置函数：</p>
<ul>
<li>16.5.1 <code>abs</code> - 求绝对值</li>
<li>16.5.2 <code>acos</code> - 反余弦</li>
<li>16.5.3 <code>acosh</code> - 反双曲余弦</li>
<li>16.5.4 <code>asin</code> - 反正弦</li>
<li>16.5.5 <code>asinh</code> - 反双曲正弦</li>
<li>16.5.6 <code>atan</code> - 反正切</li>
<li>16.5.7 <code>atanh</code> - 反双曲正切</li>
<li>16.5.8 <code>atan2</code> - 两个参数的反正切</li>
<li>16.5.9 <code>ceil</code> - 向上取整</li>
<li>16.5.10 <code>clamp</code> - 将值限制在指定范围内</li>
<li>16.5.11 <code>cos</code> - 余弦</li>
<li>16.5.12 <code>cosh</code> - 双曲余弦</li>
<li>16.5.13 <code>countLeadingZeros</code> - 统计前导零位数</li>
<li>16.5.14 <code>countOneBits</code> - 统计位中的1的数量</li>
<li>16.5.15 <code>countTrailingZeros</code> - 统计尾随零位数</li>
<li>16.5.16 <code>cross</code> - 计算叉积</li>
<li>16.5.17 <code>degrees</code> - 弧度转角度</li>
<li>16.5.18 <code>determinant</code> - 计算矩阵行列式</li>
<li>16.5.19 <code>distance</code> - 计算两点间距离</li>
<li>16.5.20 <code>dot</code> - 计算点积</li>
<li>16.5.21 <code>exp</code> - 指数函数</li>
<li>16.5.22 <code>exp2</code> - 2的幂次方</li>
<li>16.5.23 <code>extractBits (signed)</code> - 从带符号整数中提取指定位</li>
<li>16.5.24 <code>extractBits (unsigned)</code> - 从无符号整数中提取指定位</li>
<li>16.5.25 <code>faceForward</code> - 返回与朝向相反的向量</li>
<li>16.5.26 <code>firstLeadingBit (signed)</code> - 返回带符号整数中第一个前导位</li>
<li>16.5.27 <code>firstLeadingBit (unsigned)</code> - 返回无符号整数中第一个前导位</li>
<li>16.5.28 <code>firstTrailingBit</code> - 返回第一个尾随位</li>
<li>16.5.29 <code>floor</code> - 向下取整</li>
<li>16.5.30 <code>fma</code> - 乘加运算</li>
<li>16.5.31 <code>fract</code> - 返回小数部分</li>
<li>16.5.32 <code>frexp</code> - 拆分为尾数和指数</li>
<li>16.5.33 <code>insertBits</code> - 插入指定位到整数</li>
<li>16.5.34 <code>inverseSqrt</code> - 平方根的倒数</li>
<li>16.5.35 <code>ldexp</code> - 乘以2的幂次方</li>
<li>16.5.36 <code>length</code> - 计算向量长度</li>
<li>16.5.37 <code>log</code> - 自然对数</li>
<li>16.5.38 <code>log2</code> - 以2为底的对数</li>
<li>16.5.39 <code>max</code> - 取最大值</li>
<li>16.5.40 <code>min</code> - 取最小值</li>
<li>16.5.41 <code>mix</code> - 混合两个值</li>
<li>16.5.42 <code>modf</code> - 拆分为整数和小数部分</li>
<li>16.5.43 <code>normalize</code> - 归一化向量</li>
<li>16.5.44 <code>pow</code> - 指数幂运算</li>
<li>16.5.45 <code>quantizeToF16</code> - 转换为半精度浮点数</li>
<li>16.5.46 <code>radians</code> - 角度转弧度</li>
<li>16.5.47 <code>reflect</code> - 计算反射向量</li>
<li>16.5.48 <code>refract</code> - 计算折射向量</li>
<li>16.5.49 <code>reverseBits</code> - 反转位顺序</li>
<li>16.5.50 <code>round</code> - 四舍五入</li>
<li>16.5.51 <code>saturate</code> - 饱和运算</li>
<li>16.5.52 <code>sign</code> - 获取符号</li>
<li>16.5.53 <code>sin</code> - 正弦</li>
<li>16.5.54 <code>sinh</code> - 双曲正弦</li>
<li>16.5.55 <code>smoothstep</code> - 平滑阶梯插值</li>
<li>16.5.56 <code>sqrt</code> - 平方根</li>
<li>16.5.57 <code>step</code> - 阶梯函数</li>
<li>16.5.58 <code>tan</code> - 正切</li>
<li>16.5.59 <code>tanh</code> - 双曲正切</li>
<li>16.5.60 <code>transpose</code> - 转置矩阵</li>
<li>16.5.61 <code>trunc</code> - 截断取整</li>
</ul>
<p>16.6 导数内置函数：</p>
<ul>
<li><p>16.6.1 <code>dpdx</code> - 对x的偏导数</p>
</li>
<li><p>16.6.2 <code>dpdxCoarse</code> - 对x的粗略偏导数</p>
</li>
<li><p>16.6.3 <code>dpdxFine</code> - 对x的精细偏导数</p>
</li>
<li><p>16.6.4 <code>dpdy</code> - 对y的偏导数</p>
</li>
<li><p>16.6.5 <code>dpdyCoarse</code> - 对y的粗略偏导数</p>
</li>
<li><p>16.6.6 <code>dpdyFine</code> - 对y的精细偏导数</p>
</li>
<li><p>16.6.7 <code>fwidth</code> - 二维空间内的梯度长度</p>
</li>
<li><p>16.6.8 <code>fwidthCoarse</code> - 粗略梯度长度</p>
</li>
<li><p>16.6.9 <code>fwidthFine</code> - 精细梯度长度</p>
</li>
</ul>
<h2>将three.js的几何体引入webgpu</h2>
<p><strong>three.js使用index draw</strong></p>
<p><strong>基本构成：</strong></p>
<p>vertex:</p>
<p><img src="webgpu.assets/image-20230822153216538.png" alt="image-20230822153216538"></p>
<p>vertexindex:</p>
<p><img src="webgpu.assets/image-20230822153251695.png" alt="image-20230822153251695"></p>
<p>步骤：</p>
<pre><code class="language-typescript">    // three cube
    const cubeOfThree = new THREE.BoxGeometry(1, 1, 1, 10, 10, 10)
    let arrayFromThreeVertex = cubeOfThree.attributes.position.array
    let arrayFromThreeIndex = cubeOfThree.index.array
    let arrayFromThreeCount = cubeOfThree.index.count
    
    const vertexBufferFromThree = device.createBuffer({
        label: &#39;GPUBuffer store vertex&#39;,
        size: arrayFromThreeVertex.byteLength,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST | GPUBufferUsage.STORAGE,
    })
    const vertexindexFromThree = device.createBuffer({
        label: &#39;GPUBuffer store vertex index&#39;,
        size: arrayFromThreeIndex.byteLength,
        usage: GPUBufferUsage.INDEX | GPUBufferUsage.COPY_DST
    })
    
    device.queue.writeBuffer(vertexindexFromThree, 0, arrayFromThreeIndex)
    device.queue.writeBuffer(vertexBufferFromThree, 0, arrayFromThreeVertex)
</code></pre>
<p>合并到一个对象里：</p>
<pre><code class="language-typescript">    // three cube
    const cubeOfThree = new THREE.BoxGeometry(1, 1, 1, 10, 10, 10)
    let arrayFromThreeVertex = cubeOfThree.attributes.position.array
    let arrayFromThreeIndex = cubeOfThree.index.array
    let arrayFromThreeCount = cubeOfThree.index.count
    
    const vertexBufferFromThree = device.createBuffer({
        label: &#39;GPUBuffer store vertex&#39;,
        size: arrayFromThreeVertex.byteLength,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST | GPUBufferUsage.STORAGE,
    })
    const vertexindexFromThree = device.createBuffer({
        label: &#39;GPUBuffer store vertex index&#39;,
        size: arrayFromThreeIndex.byteLength,
        usage: GPUBufferUsage.INDEX | GPUBufferUsage.COPY_DST
    })
    
    device.queue.writeBuffer(vertexindexFromThree, 0, arrayFromThreeIndex)
    device.queue.writeBuffer(vertexBufferFromThree, 0, arrayFromThreeVertex)

    const cubeThree = {
        vertexBufferFromThree,
        vertexindexFromThree,
        arrayFromThreeCount
    }
</code></pre>
<p>使用<strong>topology: &#39;line-list&#39;,</strong></p>
<p><img src="webgpu.assets/image-20230822153849548.png" alt="image-20230822153849548"></p>
<p><img src="webgpu.assets/image-20230822153954364.png" alt="image-20230822153954364"></p>
<p>draw:</p>
<pre><code class="language-typescript">// set threebox vertex
passEncoder.setVertexBuffer(0, pipelineObj.vertexBufferFromThree)
passEncoder.setIndexBuffer(pipelineObj.vertexindexFromThree, &#39;uint16&#39;)
passEncoder.drawIndexed(pipelineObj.arrayFromThreeCount)
</code></pre>
<h3>对应的Layout</h3>
<p>顶点数比较多的时候就需要用到<strong>type: &#39;read-only-storage&#39;</strong></p>
<pre><code class="language-typescript">    const uniformBindGroupLayout = device.createBindGroupLayout({
        entries: [
          {
            binding: 0,
            visibility: GPUShaderStage.VERTEX,
            buffer: {
              type: &#39;uniform&#39;,
            },
          },
          {
            binding: 1,
            visibility: GPUShaderStage.VERTEX,
            buffer: {
                type: &#39;read-only-storage&#39;,
              },
          },
          {
            binding: 2,
            visibility: GPUShaderStage.VERTEX,
            buffer: {
                type: &#39;uniform&#39;,
              },
          }
        ],
      });
      const pipeLineLayout = device.createPipelineLayout({
        bindGroupLayouts: [uniformBindGroupLayout], // Add other layouts if needed
      });
</code></pre>
<h3>加入normal,uv</h3>
<pre><code class="language-typescript">buffers: [{
                arrayStride: 8 * 4, // 3 position 3 normal 2 uv,
                attributes: [
                    {
                        // position
                        shaderLocation: 0,
                        offset: 0,
                        format: &#39;float32x3&#39;,
                    },
                    {
                        // normal
                        shaderLocation: 1,
                        offset: 3 * 4,
                        format: &#39;float32x3&#39;,
                    },
                    {
                        // uv
                        shaderLocation: 2,
                        offset: 6 * 4,
                        format: &#39;float32x2&#39;,
                    }
                ]
            }]
</code></pre>
<pre><code class="language-typescript">    // three cube
    // const cubeOFThree = new THREE.BoxGeometry(1, 1, 1, 10, 10, 10)
    // const cubeOFThree = new THREE.CapsuleGeometry( 1, 1, 4, 8 )
    // const cubeOFThree = new THREE.PlaneGeometry( 2, 2, 16, 16 )
    // const cubeOFThree = new THREE.BoxGeometry(1, 1, 1)
    const cubeOFThree = new THREE.SphereGeometry( 1, 32, 16 )
    let arrayFromThreeNormal = cubeOFThree.attributes.normal.array
    let arrayFromThreeUV = cubeOFThree.attributes.uv.array
    let arrayFromThreePosition = cubeOFThree.attributes.position.array
    let arrayFromThreeCount = cubeOFThree.attributes.position.count
    let input = new Float32Array(arrayFromThreeCount * 8)
    
    for(let i = 0; i &lt; arrayFromThreeCount; i++){
        input[i*8 + 0] = arrayFromThreePosition[i*3 + 0]
        input[i*8 + 1] = arrayFromThreePosition[i*3 + 1]
        input[i*8 + 2] = arrayFromThreePosition[i*3 + 2]
        input[i*8 + 3] = arrayFromThreeNormal[i*3 + 0]
        input[i*8 + 4] = arrayFromThreeNormal[i*3 + 1]
        input[i*8 + 5] = arrayFromThreeNormal[i*3 + 2]
        input[i*8 + 6] = arrayFromThreeUV[i*2 + 0]
        input[i*8 + 7] = arrayFromThreeUV[i*2 + 1]
    }

    let arrayFromThreeIndex = cubeOFThree.index.array
    let arrayFromThreeIndexCount = cubeOFThree.index.count
    const vertexBufferFromThree = device.createBuffer({
        label: &#39;GPUBuffer store vertex&#39;,
        size: input.byteLength,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST | GPUBufferUsage.STORAGE,
    })
    const vertexindexFromThree = device.createBuffer({
        label: &#39;GPUBuffer store vertex index&#39;,
        size: arrayFromThreeIndex.byteLength,
        usage: GPUBufferUsage.INDEX | GPUBufferUsage.COPY_DST
    })
    device.queue.writeBuffer(vertexindexFromThree, 0, arrayFromThreeIndex)
    device.queue.writeBuffer(vertexBufferFromThree, 0, input)
</code></pre>
<pre><code class="language-typescript">// set threebox vertex
passEncoder.setVertexBuffer(0, pipelineObj.vertexBufferFromThree)
passEncoder.setIndexBuffer(pipelineObj.vertexindexFromThree, &#39;uint16&#39;)
passEncoder.drawIndexed(pipelineObj.arrayFromThreeIndexCount)
</code></pre>
<h1>易错</h1>
<p><img src="webgpu.assets/image-20231215175359974.png" alt="image-20231215175359974"></p>
<p><img src="webgpu.assets/image-20231217192144761.png" alt="image-20231217192144761">
<img src="webgpu.assets/image-20231218101009785.png" alt="image-20231218101009785"></p>
<p>对齐</p>
<p><img src="webgpu.assets/image-20231218215656153.png" alt="image-20231218215656153"></p>
<p>像素坐标齐次化分母</p>
<p><img src="webgpu.assets/image-20231221113231961.png" alt="image-20231221113231961"></p>
<h1>webgpu-ray-tracing-demo笔记</h1>
<ul>
<li><p>center: Float32Array后面不加[]</p>
</li>
<li><p>球心是个数组，别特么当成一个数</p>
</li>
<li><p><img src="webgpu.assets/image-20231218103947932.png" alt="image-20231218103947932"></p>
<p>解决方法是前面显示的加上类型</p>
<pre><code class="language-js">this.adapter = &lt;GPUAdapter&gt; await navigator.gpu?.requestAdapter()
</code></pre>
</li>
<li><p>解决类中异步事件的undefined问题：</p>
<pre><code class="language-js">const init = async () =&gt; {
    const canvas : HTMLCanvasElement = &lt;HTMLCanvasElement&gt; document.getElementById(&quot;gfx-main&quot;);
    
    const scene: Scene = new Scene(canvas);
    
    const renderer = new Renderer(canvas, scene);
    
    await renderer.Initialize();

    renderer.render()
}

init()
</code></pre>
</li>
<li><p><code>t &gt; tMin &amp;&amp; t &lt; tMax</code>的作用</p>
<ul>
<li><p><strong>防止光线与自身相交：</strong></p>
<ul>
<li>当光线与场景中的物体相交时，可能会在很短的距离内就与其他物体再次相交，这可能是因为浮点数计算的精度问题。通过设置一个较小的 <code>tMin</code> 值，可以避免光线与自身相交。</li>
</ul>
</li>
<li><p><strong>限制光线的检测范围：</strong></p>
<ul>
<li>有时，我们可能只对光线与物体相交的某个范围感兴趣，例如，我们可能只关心离观察者比较近的物体。通过设置 <code>tMax</code> 的值，可以限制检测的范围，减少不必要的计算。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>在给定的条件下，只有当交点距离 <code>t</code> 大于 <code>tMin</code> 且小于 <code>tMax</code> 时，函数才会认为光线与物体相交。这可以确保仅考虑感兴趣的交点，而排除不相关的情况。</p>
<pre><code>dkd+fitFn_res56_res20.sh
</code></pre>
<ul>
<li>Float32 = 32bit, 也就是4byte,也就是4B,</li>
</ul>
<p><img src="webgpu.assets/image-20231220111231261.png" alt="image-20231220111231261"></p>
<h1>免费OBJ模型</h1>
<pre><code>// 免费OBJ模型 
https://free3d.com/zh/3d-models/
</code></pre>
<h2>P20-构建仅包含球体BVH</h2>
<ul>
<li>使用递归</li>
</ul>
<p>在完成初始化所有球体的物理信息后，即开始构建BVH树	<strong>this.buildBVH()</strong></p>
<p><strong>this.buildBVH()</strong></p>
<ul>
<li><p>初始化 </p>
<ul>
<li><p><strong>包含所有包围盒的数组</strong>，要考虑到物体分布十分均匀的情况，所以设置其长度为节点的 2倍减一，每个元素都代表一个包围盒的信息</p>
<pre><code class="language-ts">export class Node {
    minCorner: vec3	// 最小边界
    leftChild: number	//左子结点的索引
    maxCorner: vec3	// 最大边界
    sphereCount: number	// 非叶子节点就是0，叶子结点就是该包围盒中的球体数目，在着色器里用来帮助判断
}
</code></pre>
<ul>
<li><p><strong>leftChild</strong></p>
<ul>
<li><p><strong>在非叶子节点时（非叶子包围盒节点）：</strong></p>
<p><code>leftChild</code> 存储的是左子节点的索引，即指向包围盒树中的下一个节点。</p>
</li>
<li><p><strong>在叶子节点时（对应球体的包围盒）：</strong></p>
<p><code>leftChild</code> 存储的是当前叶子节点所包含的球体的起始索引。</p>
</li>
</ul>
</li>
<li><p><strong>sphereCount</strong></p>
<ul>
<li><strong>非叶子节点</strong>：0</li>
<li><strong>叶子结点</strong>：就是该包围盒中的球体数目，在着色器里用来帮助判断</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>包含所有球体索引的数组</strong>，在整个构建流程会使用排序算法，将该数组中的球体按照其边界从小到大排列</p>
</li>
</ul>
</li>
</ul>
<p><strong>updateBounds(nodeIndex: number)</strong></p>
<ul>
<li>求得对应索引的包围盒节点的最大和最小边界</li>
</ul>
<p><strong>subdivide(nodeIndex: number)</strong></p>
<ul>
<li><p>在上一步的基础之上，在求得该包围盒的最长轴，并以此为划分空间，将球体按照该边界进行快速排序，并在指正重合时停止，虽然此时并不是完全按照边界大小排序，但是可以保证在指针左边的球体都比边界小，右边都比他大</p>
</li>
<li><p>之后就是对叶子结点和非叶子节点的不同情况不同处理 <strong>sphereCount</strong> <strong>leftChild</strong> 之后递归  算法太差，没整明白</p>
<p><img src="webgpu.assets/image-20231219210120447.png" alt="image-20231219210120447"></p>
</li>
</ul>
<h3>计算管线</h3>
<p>对于每一根光线，在其碰撞次数没有达到指定值之前 <strong>for</strong>循环  <strong>trace(temp_ray)</strong></p>
<p>**fn trace(ray: Ray)**初始化了：</p>
<pre><code class="language-glsl">//Set up the Render State
var renderState: RenderState;
var hitSomething: bool = false;
var nearestHit: f32 = 9999;

var node: Node = tree.nodes[0];
var stack: array&lt;Node, 15&gt;;
var stackLocation: u32 = 0;
</code></pre>
<p><strong>stack</strong> :待处理节点所存放的位置</p>
<p><strong>stackLocation</strong>：栈顶指针</p>
<p>- </p>
<ul>
<li><p>进入<strong>while</strong>循环</p>
<pre><code>var sphereCount:u32 = u32(node.sphereCount);
var content: u32 = u32(node.leftChild);
</code></pre>
</li>
<li><p>如果 <strong>sphereCount == 0</strong> 这代表当前包围盒并不是叶子结点，于是求出光线和两个孩子的距离 </p>
<p>使用函数 <strong>fn hit_aabb(ray: Ray, node: Node)求出管线到包围盒的距离(时间)：</strong></p>
<pre><code class="language-glsl">fn hit_aabb(ray: Ray, node: Node) -&gt; f32 {
    var inverseDir: vec3&lt;f32&gt; = vec3(1.0) / ray.direction;
    var t1: vec3&lt;f32&gt; = (node.minCorner - ray.origin) * inverseDir;
    var t2: vec3&lt;f32&gt; = (node.maxCorner - ray.origin) * inverseDir;
    var tMin: vec3&lt;f32&gt; = min(t1, t2); // 取所有维度的最小值
    var tMax: vec3&lt;f32&gt; = max(t1, t2);  // 取所有维度的最大值

    // 应该是最小进入时间的所有维度的最大值
    var t_min: f32 = max(max(tMin.x, tMin.y), tMin.z); 
    // 应该是最大离开时间的所有维度的最小值
    var t_max: f32 = min(min(tMax.x, tMax.y), tMax.z);

    // 这两种情况代表没有击中或者盒子处于射线背面
    if (t_min &gt; t_max || t_max &lt; 0) {
        return 99999;
    }
    else {
        // 返回最小距离，我们只需要这个
        return t_min;
    }
}
</code></pre>
</li>
<li><p>我们要让第一个孩子始终是距离最小的那个</p>
<ul>
<li>所以 <strong>distance1 &gt; distance2</strong>就交换两个节点位置</li>
<li><strong>distance1 &gt; nearestHit</strong>代表已经超出最远距离，直接跳出循环，代表当前包围盒及其子节点都未击中</li>
<li>否则代表 <strong>distance1 &lt; distance2</strong> 但是这个时候，如果 <strong>distance2 &lt; nearestHit</strong>,要将 <strong>child2</strong>放入栈中，作为要探索的节点，同时栈顶指针加一</li>
</ul>
</li>
<li><p>这样当 <strong>sphereCount == 0</strong>不再满足时，就代表已经到达叶子结点了，这时候就要对该包围盒的球体遍历求交</p>
</li>
</ul>
<p>​		这里使用BVH叶子节点对应的球体:</p>
<p>​							 <strong>objects.triangles[u32(triangleLookup.primitiveIndices[i + contents])],</strong></p>
<p>​	</p>
<p>​		</p>
<h2>P24-基于三角面构建BVH</h2>
<h3>导入obj模型</h3>
<ul>
<li>注意隐藏填充</li>
</ul>
<p>我们需要的数据格式：</p>
<pre><code class="language-glsl">struct Triangle {
    corner_a: vec3&lt;f32&gt;,
    //float
    normal_a: vec3&lt;f32&gt;,
    //float
    corner_b: vec3&lt;f32&gt;,
    //float
    normal_b: vec3&lt;f32&gt;,
    //float
    corner_c: vec3&lt;f32&gt;,
    //float
    normal_c: vec3&lt;f32&gt;,
    //float
    color: vec3&lt;f32&gt;,
    //float
}
</code></pre>
<p>其中在构建BVH加速结构时，划分空间时，以三角形的重心坐标为准来划分，这里采取每个轴的平均值</p>
<p>如果模型需要进行模型变换，那么将变换矩阵也传到shaders里</p>
<p>模型进行模型变换，直接改变顶点信息是十分消耗性能的，这里采取将世界空间坐标的光线变换到对象坐标空间，</p>
<p>就是使用物体模型变换矩阵的逆矩阵：</p>
<pre><code class="language-glsl">object_ray.origin = (scene.inverseModel * vec4&lt;f32&gt;(ray.origin, 1.0)).xyz;
object_ray.direction = (scene.inverseModel * vec4&lt;f32&gt;(ray.direction, 0.0)).xyz;
</code></pre>
<p>之后使用变换过后的光线在完成一次光线相交后，得到 <strong>t</strong></p>
<ul>
<li><p><code>result.t</code> 可以被抽象为光线和物体相交的时间（或者说参数）。在光线追踪中，这个时间参数表示光线从起点沿着方向走过的距离，直到与物体相交的位置。当这个时间参数 <code>t</code> 被应用到射线方程时，它决定了射线上的点，而这个点正是与物体相交的点。</p>
<p>不论是在世界坐标系中还是在物体空间坐标系中，这个时间参数 <code>t</code> 都用于计算交点的位置。在世界坐标系中，光线方程可以表示为 <code>P(t) = O + t * D</code>，其中 <code>O</code> 是光线的起点，<code>D</code> 是光线的方向向量，而 <code>t</code> 就是时间参数。在物体空间坐标系中，这个概念同样成立，只不过方程的变换会受到模型变换矩阵的逆矩阵的影响。因此，通过时间参数 <code>t</code>，我们能够在不同坐标系中准确地表示光线和物体的交点。</p>
<pre><code class="language-glsl">world_ray.origin = world_ray.origin + result.t * world_ray.direction;
world_ray.direction = normalize(reflect(world_ray.direction, result.normal));
</code></pre>
<p>这两行代码是在世界坐标系中更新光线的起点和方向，使其表示与物体相交后的位置和反射方向。</p>
<pre><code class="language-glsl">wgslCopy codeobject_ray.origin = (scene.inverseModel * vec4&lt;f32&gt;(world_ray.origin, 1.0)).xyz;
object_ray.direction = (scene.inverseModel * vec4&lt;f32&gt;(world_ray.direction, 0.0)).xyz;
</code></pre>
<p>这两行代码是在物体空间坐标系中更新光线的起点和方向。通过将世界坐标系中的交点坐标和反射方向应用模型变换矩阵的逆矩阵，得到了物体空间坐标系中的对应值。</p>
<p>这样，你就可以在两个不同的坐标系中描述相同的物理交互。在物体空间坐标系中进行相交检测和反射计算可以简化问题，因为它相当于将光线追踪的焦点放置在物体自身。通过在不同坐标系之间转换，你可以更方便地处理光线和物体的相交关系。</p>
</li>
</ul>
<p>所以每次完成一次相交后，在返回了新的状态：</p>
<pre><code class="language-glsl">struct RenderState {
    t: f32,
    color: vec3&lt;f32&gt;,
    hit: bool,
    position: vec3&lt;f32&gt;,
    normal: vec3&lt;f32&gt;,
}
</code></pre>
<p>我们要重新基于新的状态计算碰撞后反射的光线，也就是新的射线的世界坐标系下的方程，新射线的起点直接带入即可：</p>
<p>​			<strong>world_ray.origin = world_ray.origin + result.t * world_ray.direction;</strong></p>
<p>方向则是和相交点的法线对称：
            <strong>world_ray.direction = normalize(reflect(world_ray.direction, result.normal));</strong></p>
<p>因为我们要在物体空间坐标系下进行光线追踪，所以还要将新的光线在此变换到物体的物体空间坐标系下：</p>
<pre><code class="language-glsl">object_ray.origin = (scene.inverseModel * vec4&lt;f32&gt;(world_ray.origin, 1.0)).xyz;
object_ray.direction = (scene.inverseModel * vec4&lt;f32&gt;(world_ray.direction, 0.0)).xyz;
</code></pre>
<p>之后在没有相交退出循环之后，在采样 <strong>cube Map</strong> 时，使用世界空间坐标系下的射线方向向量</p>
<pre><code class="language-glsl">textureSampleLevel(skyTexture, skySampler, world_ray.direction, 0.0).xyz
</code></pre>
<h3>光线&amp;三角形相交检测</h3>
<p>每次相交就存储碰撞距离，下一个三角形的碰撞距离小于这个就认定为未碰撞到</p>
<p>由重心坐标下的平面方程和光线方程联立，使用克拉默法则求解</p>
<p>平面法向量和光线点积后的标量是正值说明二者在同一方向，如果要使用该平面法向量参与后面对交点法向量的计算，就要将其反转，同时如果点积太小说明三角面和光线几乎平行，直接 <strong>return</strong>，当做没有碰撞到该三角面</p>
<p>计算 u ,v 时注意计算所得的值的范围判断</p>
<pre><code class="language-glsl">if (u &lt; 0.0 || u &gt; 1.0) {
    return renderState;
}
if (v &lt; 0.0 || u + v &gt; 1.0) {
    return renderState;
}
</code></pre>
<p>在由克拉默法则计算时，对分母行列式的计算： <strong>let denominator: f32 = determinant(system_matrix);</strong></p>
<p>该值太小也要当做未击中</p>
<p>在计算交点法线时，使用重心坐标插值：</p>
<pre><code class="language-glsl">let normal: vec3&lt;f32&gt; = (1.0 - u - v) * tri.normal_a + u * tri.normal_b + v * tri.normal_c;
renderState.normal = normalize((transpose(scene.inverseModel) * vec4(normal, 0.0)).xyz);
</code></pre>
<p><img src="webgpu.assets/image-20231219213520076.png" alt="image-20231219213520076"></p>
<p><img src="webgpu.assets/image-20231219211454821.png" alt="image-20231219211454821"></p>

</body>
</html>